{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ee655db-ac6b-49b8-9c7f-02d609e972a1",
   "metadata": {},
   "source": [
    "## Log sequence anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3deae48-58e4-4f40-9f0f-0414d2027118",
   "metadata": {},
   "source": [
    "### Content\n",
    "- Introduction\n",
    "- Dataset\n",
    "- Training and Evaluation\n",
    "- Conclusion\n",
    "- Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ec9cca-7cc7-4674-b77f-47f31c8baec2",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Anomaly detection in sequential log data aims to identify sequences that deviate from the expected behavior\n",
    "or patterns. For example, software intensive systems often record runtime information by\n",
    "printing console logs. A large and complex system could produce a massive amount of logs, which can be used for troubleshooting\n",
    "purposes. It is critical to detect anomalous states in a timely manner to ensure the reliability of the software system and mitigate the losses. \n",
    "\n",
    "Log data is usually unstructured text messages, which can help engineers understand the systemâ€™s internal\n",
    "status and facilitate monitoring, administering, and troubleshooting of the system log messages. The log messages can be modeled as an event sequence, where abnormality in events within a sequence could indicate abnormality in the log message.\n",
    "\n",
    "This usecase shows a workflow for identifying sequential anomalies using an example of log message dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "552e9437-779e-4e24-996c-4b27e82089d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import utils, datatools, model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "SEED = 91\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bb123e-e80b-417c-b41e-0fdd61e5e208",
   "metadata": {},
   "source": [
    "#### Dataset processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9eac0c7-4e4f-4816-8c53-09a9b7d0e138",
   "metadata": {},
   "source": [
    "The dataset for the example is from BlueGene/L Supercomputer System (BGL). BGL dataset contains 4,747,963 log messages that are collected\n",
    "from a [BlueGeme/L]('https://zenodo.org/record/3227177/files/BGL.tar.gz?download=1') supercomputer system at Lawrence Livermore National Labs. The log messages can be categorized into alert and not-alert messages. The log message is parsed using [`Drain`](https://github.com/logpai/logparser) parser into structured log format, then the structured log is used to train the model. This work is based on the model develeped in the works of [[2](https://ieeexplore.ieee.org/document/9671642),[3](https://github.com/hanxiao0607/InterpretableSAD)], for further detail refer the paper and associated code at the reference link.\n",
    "\n",
    "For running this workflow we can also use a portion of parsed BGL dataset taken from  https://github.com/LogIntelligence/LogPPT. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f87177a-69bb-418f-a75c-e92c1ca8675a",
   "metadata": {},
   "source": [
    "#### Preprocessing log dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3597fe53-60a5-4b8b-923e-9a1da9918e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/LogIntelligence/LogPPT/master/logs/BGL/BGL_2k.log_structured.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f30b690-78a6-4697-a8e3-409f21fd3869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small dataset for testing\n",
    "DATASET_NAME = 'https://raw.githubusercontent.com/LogIntelligence/LogPPT/master/logs/BGL/BGL_2k.log_structured.csv' #'BGL_2k'\n",
    "TRAIN_SIZE = 100 \n",
    "WINDOW_SIZE = 10\n",
    "STEP_SIZE = 10\n",
    "RATIO = 0.1\n",
    "\n",
    "# Full dataset parsed using DRAIN parser\n",
    "# DATASET_NAME = 'dataset/bgl_1m.log_structured.csv'\n",
    "# TRAIN_SIZE = 10000 #00\n",
    "# WINDOW_SIZE = 100\n",
    "# STEP_SIZE = 20\n",
    "# RATIO = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d54842",
   "metadata": {},
   "source": [
    "#### Problem statement\n",
    "An event set $\\mathcal{E}$ contains all possible events in the whole log message, then a log event sequence $S_i$ is defined as sequences of events $S_i = (e_1^i, e_2^i, \\ldots, e_N^i)$, where \n",
    "$e_i^j \\in \\mathcal{E}$, and $N^i$ is the length of the sequence $S^i$. Given a set of sequences $ S = \\{S^1, S^2, \\ldots, S^N\\}$, where each sequence is normal or abnormal.  Then the dataset $S$ is used to train sequence classifier for binary prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f6c5ac",
   "metadata": {},
   "source": [
    "For example, this raw structured log message shows parsed logs along with timestamp, and event template. Each line indicates an event in the log message, and a sliding window & vector representation is used to create a sequence of these messages with its vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23ac4a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LineId</th>\n",
       "      <th>Label</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Date</th>\n",
       "      <th>Node</th>\n",
       "      <th>Time</th>\n",
       "      <th>NodeRepeat</th>\n",
       "      <th>Type</th>\n",
       "      <th>Component</th>\n",
       "      <th>Level</th>\n",
       "      <th>Content</th>\n",
       "      <th>EventId</th>\n",
       "      <th>EventTemplate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>1117838570</td>\n",
       "      <td>2005.06.03</td>\n",
       "      <td>R02-M1-N0-C:J12-U11</td>\n",
       "      <td>2005-06-03-15.42.50.675872</td>\n",
       "      <td>R02-M1-N0-C:J12-U11</td>\n",
       "      <td>RAS</td>\n",
       "      <td>KERNEL</td>\n",
       "      <td>INFO</td>\n",
       "      <td>instruction cache parity error corrected</td>\n",
       "      <td>E77</td>\n",
       "      <td>instruction cache parity error corrected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-</td>\n",
       "      <td>1117838573</td>\n",
       "      <td>2005.06.03</td>\n",
       "      <td>R02-M1-N0-C:J12-U11</td>\n",
       "      <td>2005-06-03-15.42.53.276129</td>\n",
       "      <td>R02-M1-N0-C:J12-U11</td>\n",
       "      <td>RAS</td>\n",
       "      <td>KERNEL</td>\n",
       "      <td>INFO</td>\n",
       "      <td>instruction cache parity error corrected</td>\n",
       "      <td>E77</td>\n",
       "      <td>instruction cache parity error corrected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-</td>\n",
       "      <td>1117838976</td>\n",
       "      <td>2005.06.03</td>\n",
       "      <td>R02-M1-N0-C:J12-U11</td>\n",
       "      <td>2005-06-03-15.49.36.156884</td>\n",
       "      <td>R02-M1-N0-C:J12-U11</td>\n",
       "      <td>RAS</td>\n",
       "      <td>KERNEL</td>\n",
       "      <td>INFO</td>\n",
       "      <td>instruction cache parity error corrected</td>\n",
       "      <td>E77</td>\n",
       "      <td>instruction cache parity error corrected</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LineId Label   Timestamp        Date                 Node  \\\n",
       "0       1     -  1117838570  2005.06.03  R02-M1-N0-C:J12-U11   \n",
       "1       2     -  1117838573  2005.06.03  R02-M1-N0-C:J12-U11   \n",
       "2       3     -  1117838976  2005.06.03  R02-M1-N0-C:J12-U11   \n",
       "\n",
       "                         Time           NodeRepeat Type Component Level  \\\n",
       "0  2005-06-03-15.42.50.675872  R02-M1-N0-C:J12-U11  RAS    KERNEL  INFO   \n",
       "1  2005-06-03-15.42.53.276129  R02-M1-N0-C:J12-U11  RAS    KERNEL  INFO   \n",
       "2  2005-06-03-15.49.36.156884  R02-M1-N0-C:J12-U11  RAS    KERNEL  INFO   \n",
       "\n",
       "                                    Content EventId  \\\n",
       "0  instruction cache parity error corrected     E77   \n",
       "1  instruction cache parity error corrected     E77   \n",
       "2  instruction cache parity error corrected     E77   \n",
       "\n",
       "                              EventTemplate  \n",
       "0  instruction cache parity error corrected  \n",
       "1  instruction cache parity error corrected  \n",
       "2  instruction cache parity error corrected  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318e3c3f",
   "metadata": {},
   "source": [
    "The `WINDOW_SIZE` indicates the sliding window size or the sequence length. The `STEP_SIZE` indicate the overlap of events across consecutive sequence within the window size. If `WINDOW_SIZE` is equal to `STEP_SIZE`, then events are not overlapped. The value of window size and step size are parameters chosen based on the log dataset size and the sequence length we want to give as input to the model. In this case, the parameters are chosen based on the empirical result provided in the referenced paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e0926c",
   "metadata": {},
   "source": [
    "The `sliding_window` method return word2vector transformed vector representation of the log sequences of the training set, testing set, weight vector, and bigram in the training set. The bigram is \n",
    "used later for generating negative samples. The `w2v_dict` is used to lookup for test data logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a98e694-2dc5-4d1b-ab56-e26b9256ab71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: dataset/bgl_1m.log_structured.csv\n",
      "Total logs in the dataset:  1000000\n",
      "training size 10000\n",
      "test normal size 26631\n",
      "test abnormal size 13365\n",
      "Number of training keys: 93\n",
      "Word2Vec model: Word2Vec(vocab=94, size=8, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# Create train and test dataset by transforming log dataset into embedding vectors of train, test set and associated word2vector weights.\n",
    "train_normal, test_normal, test_abnormal, bigram, unique, weights, train_dict, w2v_dict = datatools.sliding_window(DATASET_NAME, WINDOW_SIZE, STEP_SIZE, TRAIN_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27474c60-b43c-4bf0-bc1b-25580618a0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparmeters\n",
    "vocab_dim = len(train_dict)+1\n",
    "output_dim = 2\n",
    "emb_dim = 8\n",
    "hidden_dim = 128\n",
    "n_layers = 1\n",
    "dropout = 0.0\n",
    "batch_size = 32\n",
    "times = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bfb858-d434-48dd-8081-899db08440f2",
   "metadata": {},
   "source": [
    "Given a set of normal sequences, anomalous sequences are generated via negative sampling. Negative sampling generates anomalous samples by randomly replacing $n$ number of events in sequence $s_i$. A randomly selected event $e_{t+1}$ in sequence $s_i=(e_t, e_{t+1})$ is replaced with an event $e_{t*+1}$ so that the bigram $(e_t, e_{t*+1})$ is rare event in the training set. This introduces suspicious events with low frequency, then we expect that there is a high probability that the generated event sequence is anomalous. An LSTM sequence classifier is trained to classify the negative samples from the true positive samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd319ea5-b287-415b-8e1d-ea4e6ca83cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate negative samples and split into training data and validation data. \n",
    "neg_samples = datatools.negative_sampling(train_normal, bigram, unique, times, vocab_dim)\n",
    "df_neg = datatools.get_dataframe(neg_samples, 1, w2v_dict)\n",
    "df_pos = datatools.get_dataframe(list(train_normal['EventId']), 0, w2v_dict)\n",
    "df_pos.columns = df_pos.columns.astype(str)\n",
    "df_train = pd.concat([df_pos, df_neg], ignore_index = True, axis=0)\n",
    "df_train.reset_index(drop = True)\n",
    "y = list(df_train.loc[:,'class_label'])\n",
    "X = list(df_train['W2V_EventId'])\n",
    "\n",
    "# split train, validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = torch.tensor(X_train,requires_grad=False).long()\n",
    "X_val = torch.tensor(X_val,requires_grad=False).long()\n",
    "y_train = torch.tensor(y_train).reshape(-1, 1).long()\n",
    "y_val = torch.tensor(y_val).reshape(-1, 1).long()\n",
    "train_iter = utils.get_iter(X_train, y_train, batch_size)\n",
    "val_iter = utils.get_iter(X_val, y_val, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e8c92b-d049-4194-8bdd-cf68c3b72655",
   "metadata": {},
   "source": [
    "### Training and Evaluation\n",
    "\n",
    "An LSTM model is trained using word2vector input generated from both positive and negative examples with task of binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b64d0c6-ab38-4aac-9af8-475bc84e3134",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device( \"cuda\" if torch.cuda.is_available() else\"cpu\")\n",
    "n_epoch = 10\n",
    "kwargs = {\"matrix_embeddings\":weights, \n",
    "\"vocab_dim\": vocab_dim, \"output_dim\": output_dim, \"emb_dim\": emb_dim,\n",
    "\"hid_dim\": hidden_dim, \n",
    "\"n_layers\": n_layers, \n",
    "\"dropout\": dropout}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "661c5896-e3e4-4f28-ae39-035b1e5ccd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 1/10 [00:37<05:35, 37.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 37s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.036 |  Val. PPL:   1.037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 2/10 [01:12<04:48, 36.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 0m 35s\n",
      "\tTrain Loss: 0.034 | Train PPL:   1.035\n",
      "\t Val. Loss: 0.031 |  Val. PPL:   1.031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [01:49<04:14, 36.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Time: 0m 36s\n",
      "\tTrain Loss: 0.032 | Train PPL:   1.032\n",
      "\t Val. Loss: 0.032 |  Val. PPL:   1.033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [02:25<03:38, 36.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Time: 0m 36s\n",
      "\tTrain Loss: 0.030 | Train PPL:   1.031\n",
      "\t Val. Loss: 0.031 |  Val. PPL:   1.032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [03:02<03:02, 36.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Time: 0m 36s\n",
      "\tTrain Loss: 0.030 | Train PPL:   1.030\n",
      "\t Val. Loss: 0.032 |  Val. PPL:   1.032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [03:39<02:26, 36.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Time: 0m 36s\n",
      "\tTrain Loss: 0.030 | Train PPL:   1.030\n",
      "\t Val. Loss: 0.028 |  Val. PPL:   1.028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [04:16<01:50, 36.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Time: 0m 36s\n",
      "\tTrain Loss: 0.028 | Train PPL:   1.029\n",
      "\t Val. Loss: 0.026 |  Val. PPL:   1.027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [04:52<01:13, 36.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Time: 0m 36s\n",
      "\tTrain Loss: 0.024 | Train PPL:   1.024\n",
      "\t Val. Loss: 0.023 |  Val. PPL:   1.023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [05:29<00:36, 36.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Time: 0m 36s\n",
      "\tTrain Loss: 0.025 | Train PPL:   1.025\n",
      "\t Val. Loss: 0.022 |  Val. PPL:   1.022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [06:06<00:00, 36.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Time: 0m 36s\n",
      "\tTrain Loss: 0.020 | Train PPL:   1.020\n",
      "\t Val. Loss: 0.021 |  Val. PPL:   1.021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "LAD_model = model.LogLSTM(weights, vocab_dim, output_dim, emb_dim, hidden_dim, n_layers, dropout).to(device)\n",
    "optimizer = optim.Adam(LAD_model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "try:\n",
    "    os.makedirs('model')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Train LSTM model\n",
    "clip = 1\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "for epoch in tqdm(range(n_epoch)):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_loss= model.train(LAD_model, train_iter, optimizer, criterion,  device)        \n",
    "\n",
    "    val_loss = model.evaluate(LAD_model, val_iter, criterion, device)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = model.epoch_time(start_time, end_time)\n",
    "    \n",
    "    if val_loss < best_test_loss:\n",
    "        best_test_loss = val_loss\n",
    "        torch.save({\n",
    "            'model_state_dict':LAD_model.state_dict(),\n",
    "            \"model_hyperparam\": kwargs,\n",
    "            \"W2V_conf\": {\n",
    "            'train_dict': train_dict, \n",
    "            'w2v_dict': w2v_dict,\n",
    "            \"WINDOW_SIZE\": WINDOW_SIZE,\n",
    "            \"STEP_SIZE\": STEP_SIZE\n",
    "            }\n",
    "        }, 'model/model_BGL.pt')\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. PPL: {math.exp(val_loss):7.3f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79d43d0-011d-4f62-b8c8-f2f2cbcc05df",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Since the test cases are imbalanced dataset, we use precision-recall and F1 score of the test sample to evaluate the model performance. Overall, F1 gives general overview performance of the model over recall/precision criteria of the test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42b9c65a-b63d-4260-90ac-226ebef8edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data proportion\n",
    "test_abnormal_ratio = model.ratio_abnormal_sequence(test_abnormal, WINDOW_SIZE, RATIO)\n",
    "test_ab_X, test_ab_X_key_label = test_abnormal_ratio['W2V_EventId'], test_abnormal_ratio['Key_label']\n",
    "test_n_X, test_n_X_key_label = test_normal['W2V_EventId'], test_normal['Key_label']\n",
    "test_ab_y = test_abnormal_ratio['Label']\n",
    "test_n_y = test_normal['Label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c215abc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.98333   0.96686   0.97503      2867\n",
      "           1    0.93611   0.96734   0.95147      1439\n",
      "\n",
      "    accuracy                        0.96702      4306\n",
      "   macro avg    0.95972   0.96710   0.96325      4306\n",
      "weighted avg    0.96755   0.96702   0.96715      4306\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute evaluation metrics\n",
    "y, y_pre = model.model_precision(LAD_model, device, test_n_X.values.tolist()[:int(len(test_n_X.values.tolist())*(len(test_abnormal_ratio)/len(test_abnormal)))], \\\n",
    "                           test_ab_X.values.tolist())\n",
    "f1_acc = metrics.classification_report(y, y_pre, digits=5)\n",
    "print(f1_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4731f3f",
   "metadata": {},
   "source": [
    "To perform inference we can load a saved model and perform inference as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dcd6ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load trained model and parameters for inference\n",
    "\n",
    "check_point  = torch.load('model/model_BGL.pt')\n",
    "\n",
    "window_df = datatools.preprocess(df, check_point['W2V_conf']['WINDOW_SIZE'], check_point['W2V_conf']['STEP_SIZE'])\n",
    "\n",
    "\n",
    "# # convert to input vector\n",
    "test_vector = datatools.test_vector(window_df, check_point['W2V_conf']['train_dict'], check_point['W2V_conf']['w2v_dict'])\n",
    "\n",
    "# # load LogLSTM model\n",
    "trained_model_ = model.LogLSTM(**check_point['model_hyperparam']).to(device)\n",
    "trained_model_.load_state_dict(check_point['model_state_dict'])\n",
    "\n",
    "# # predict label\n",
    "_, y_pred = model.model_inference(trained_model_, device, test_vector['W2V_EventId'].values.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980be0d1-edd9-4c02-a52f-4e75a67c4600",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this workflow, we show a pipeline for training sequence binary classifier to identify anomalous log sequence from set of generated log sequences. We used negative sampling to generate negative examples along normal logs for training the model. The model is evaluated on BGL dataset to identify alerts from non-alert messages. With an F1 score of 0.9 the model is able to identify true alerts from non-alert messages of test log samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96aa849-7069-4f0a-94cb-7afe6b4e4496",
   "metadata": {},
   "source": [
    "### Reference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0735f94a-1ccb-4a50-a754-3fbcd2b10b96",
   "metadata": {},
   "source": [
    "1. https://arxiv.org/pdf/2202.04301.pdf\n",
    "2. https://ieeexplore.ieee.org/document/9671642\n",
    "3.  https://github.com/hanxiao0607/InterpretableSAD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "f708a36acfaef0acf74ccd43dfb58100269bf08fb79032a1e0a6f35bd9856f51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
