{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ee655db-ac6b-49b8-9c7f-02d609e972a1",
   "metadata": {},
   "source": [
    "## Log sequence anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3deae48-58e4-4f40-9f0f-0414d2027118",
   "metadata": {},
   "source": [
    "### Content\n",
    "- Introduction\n",
    "- Dataset\n",
    "- Training and Evaluation\n",
    "- Conclusion\n",
    "- Reference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92ec9cca-7cc7-4674-b77f-47f31c8baec2",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Anomaly detection in sequential log data aims to identify sequences that deviate from the expected behavior\n",
    "or patterns. For example, software intensive systems often record runtime information by\n",
    "printing console logs. A large and complex system could produce a massive amount of logs, which can be used for troubleshooting\n",
    "purposes. The log messages can be modeled as an event sequence. It is critical to detect anomalous states\n",
    "in a timely manner to ensure the reliability the software system and mitigate the losses.\n",
    "\n",
    "Log data is usually unstructured text messages, which can help engineers understand the system’s internal\n",
    "status and facilitate monitoring, administering, and troubleshooting of the system Log messages can be parsed into log events,\n",
    "which are templates (constant part) of the messages. \n",
    " \n",
    "This usecase shows a workflow for identifying sequential anomalies from raw log sequence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "552e9437-779e-4e24-996c-4b27e82089d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import utils, datatools, model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "SEED = 91\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bb123e-e80b-417c-b41e-0fdd61e5e208",
   "metadata": {},
   "source": [
    "#### Dataset processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9eac0c7-4e4f-4816-8c53-09a9b7d0e138",
   "metadata": {},
   "source": [
    "The dataset for the example used from BlueGene/L Supercomputer System (BGL). BGL dataset contains 4,747,963 log messages that are collected\n",
    "from a [BlueGeme/L]('https://zenodo.org/record/3227177/files/BGL.tar.gz?download=1') supercomputer system at Lawrence Livermore National Labs. The log messages can be categorized into alert and not-alert messages. The log message is parsed using [`Drain`](https://github.com/logpai/logparser) parser into structured log format.\n",
    "\n",
    "For running this workflow we can use portion of parsed BGL dataset taken from  https://github.com/LogIntelligence/LogPPT. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f87177a-69bb-418f-a75c-e92c1ca8675a",
   "metadata": {},
   "source": [
    "#### Preprocessing log dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3597fe53-60a5-4b8b-923e-9a1da9918e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/LogIntelligence/LogPPT/master/logs/BGL/BGL_2k.log_structured.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f30b690-78a6-4697-a8e3-409f21fd3869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small dataset for testing\n",
    "DATASET_NAME = 'https://raw.githubusercontent.com/LogIntelligence/LogPPT/master/logs/BGL/BGL_2k.log_structured.csv' #'BGL_2k'\n",
    "TRAIN_SIZE = 100 \n",
    "WINDOW_SIZE = 10\n",
    "STEP_SIZE = 20\n",
    "RATIO = 0.1\n",
    "\n",
    "# Full dataset parsed using DRAIN parser\n",
    "# DATASET_NAME = 'dataset/bgl_1m.log_structured.csv'\n",
    "# TRAIN_SIZE = 10000 #00\n",
    "# WINDOW_SIZE = 100\n",
    "# STEP_SIZE = 20\n",
    "# RATIO = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e0926c",
   "metadata": {},
   "source": [
    "Create train and test dataset by transforming log dataset into embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a98e694-2dc5-4d1b-ab56-e26b9256ab71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: dataset/bgl_1m.log_structured.csv\n",
      "Total logs in the dataset:  1000000\n",
      "training size 10000\n",
      "test normal size 26631\n",
      "test abnormal size 13365\n",
      "Number of training keys: 93\n",
      "Word2Vec model: Word2Vec(vocab=94, size=8, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "train_normal, test_normal, test_abnormal, bigram, unique, weights, train_dict, w2v_dict = datatools.sliding_window(DATASET_NAME, WINDOW_SIZE, STEP_SIZE, TRAIN_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27474c60-b43c-4bf0-bc1b-25580618a0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparmeters\n",
    "vocab_dim = len(train_dict)+1\n",
    "output_dim = 2\n",
    "emb_dim = 8\n",
    "hidden_dim = 128\n",
    "n_layers = 1\n",
    "dropout = 0.0\n",
    "batch_size = 32\n",
    "times = 20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40bfb858-d434-48dd-8081-899db08440f2",
   "metadata": {},
   "source": [
    "Generate negative samples and split into training data and validation data. Given a set of normal sequences, an anomalous sequences are generated via neative sampling. A binary sequence classifier is trained to classify the negative samples from the true positive samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd319ea5-b287-415b-8e1d-ea4e6ca83cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_samples = datatools.negative_sampling(train_normal, bigram, unique, times, vocab_dim)\n",
    "df_neg = datatools.get_dataframe(neg_samples, 1, w2v_dict)\n",
    "df_pos = datatools.get_dataframe(list(train_normal['EventId']), 0, w2v_dict)\n",
    "df_pos.columns = df_pos.columns.astype(str)\n",
    "df_train = pd.concat([df_pos, df_neg], ignore_index = True, axis=0)\n",
    "df_train.reset_index(drop = True)\n",
    "y = list(df_train.loc[:,'class_label'])\n",
    "X = list(df_train['W2V_EventId'])\n",
    "\n",
    "# split train, validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = torch.tensor(X_train,requires_grad=False).long()\n",
    "X_val = torch.tensor(X_val,requires_grad=False).long()\n",
    "y_train = torch.tensor(y_train).reshape(-1, 1).long()\n",
    "y_val = torch.tensor(y_val).reshape(-1, 1).long()\n",
    "train_iter = utils.get_iter(X_train, y_train, batch_size)\n",
    "val_iter = utils.get_iter(X_val, y_val, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6e171dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventId</th>\n",
       "      <th>class_label</th>\n",
       "      <th>W2V_EventId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[29, 22, 29, 22, 29, 22, 29, 22, 29, 22, 29, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[3, 4, 4, 3, 4, 3, 4, 3, 4, 3, 3, 4, 3, 4, 3, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[4, 10, 10, 4, 10, 4, 10, 4, 10, 4, 4, 10, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[5, 6, 5, 6, 5, 6, 6, 5, 6, 5, 6, 5, 6, 5, 6, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[23, 39, 23, 39, 23, 39, 39, 23, 39, 23, 39, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209995</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209996</th>\n",
       "      <td>[22, 22, 22, 15, 89, 29, 22, 22, 22, 82, 22, 2...</td>\n",
       "      <td>1</td>\n",
       "      <td>[28, 28, 28, 56, 84, 30, 28, 28, 28, 87, 28, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209997</th>\n",
       "      <td>[56, 30, 56, 30, 56, 56, 30, 56, 30, 56, 30, 5...</td>\n",
       "      <td>1</td>\n",
       "      <td>[32, 26, 32, 26, 32, 32, 26, 32, 26, 32, 26, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209998</th>\n",
       "      <td>[0, 0, 0, 0, 0, 2, 0, 0, 0, 51, 0, 0, 0, 0, 91...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 22, 0, 0, 0, 17, 0, 0, 0, 0, 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209999</th>\n",
       "      <td>[59, 0, 89, 0, 41, 0, 0, 0, 36, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>1</td>\n",
       "      <td>[69, 0, 84, 0, 71, 0, 0, 0, 16, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  EventId  class_label  \\\n",
       "0       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...            0   \n",
       "1       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...            0   \n",
       "2       [1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, ...            0   \n",
       "3       [3, 4, 4, 3, 4, 3, 4, 3, 4, 3, 3, 4, 3, 4, 3, ...            0   \n",
       "4       [5, 6, 5, 6, 5, 6, 6, 5, 6, 5, 6, 5, 6, 5, 6, ...            0   \n",
       "...                                                   ...          ...   \n",
       "209995  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...            1   \n",
       "209996  [22, 22, 22, 15, 89, 29, 22, 22, 22, 82, 22, 2...            1   \n",
       "209997  [56, 30, 56, 30, 56, 56, 30, 56, 30, 56, 30, 5...            1   \n",
       "209998  [0, 0, 0, 0, 0, 2, 0, 0, 0, 51, 0, 0, 0, 0, 91...            1   \n",
       "209999  [59, 0, 89, 0, 41, 0, 0, 0, 36, 0, 0, 0, 0, 0,...            1   \n",
       "\n",
       "                                              W2V_EventId  \n",
       "0       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2       [29, 22, 29, 22, 29, 22, 29, 22, 29, 22, 29, 2...  \n",
       "3       [4, 10, 10, 4, 10, 4, 10, 4, 10, 4, 4, 10, 4, ...  \n",
       "4       [23, 39, 23, 39, 23, 39, 39, 23, 39, 23, 39, 2...  \n",
       "...                                                   ...  \n",
       "209995  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "209996  [28, 28, 28, 56, 84, 30, 28, 28, 28, 87, 28, 2...  \n",
       "209997  [32, 26, 32, 26, 32, 32, 26, 32, 26, 32, 26, 3...  \n",
       "209998  [0, 0, 0, 0, 0, 22, 0, 0, 0, 17, 0, 0, 0, 0, 8...  \n",
       "209999  [69, 0, 84, 0, 71, 0, 0, 0, 16, 0, 0, 0, 0, 0,...  \n",
       "\n",
       "[210000 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32e8c92b-d049-4194-8bdd-cf68c3b72655",
   "metadata": {},
   "source": [
    "### Training and Evaluation\n",
    "\n",
    "An LSTM model is trained using word2vector input genrated from both positive and negative examples with task of binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "661c5896-e3e4-4f28-ae39-035b1e5ccd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [06:04<54:43, 364.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 6m 4s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.027 |  Val. PPL:   1.027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [12:08<48:35, 364.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 6m 4s\n",
      "\tTrain Loss: 0.024 | Train PPL:   1.024\n",
      "\t Val. Loss: 0.021 |  Val. PPL:   1.022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [18:15<42:37, 365.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Time: 6m 6s\n",
      "\tTrain Loss: 0.019 | Train PPL:   1.020\n",
      "\t Val. Loss: 0.017 |  Val. PPL:   1.018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [24:21<36:34, 365.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Time: 6m 6s\n",
      "\tTrain Loss: 0.016 | Train PPL:   1.016\n",
      "\t Val. Loss: 0.016 |  Val. PPL:   1.016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [30:45<31:00, 372.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Time: 6m 23s\n",
      "\tTrain Loss: 0.018 | Train PPL:   1.018\n",
      "\t Val. Loss: 0.021 |  Val. PPL:   1.021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [37:07<25:02, 375.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Time: 6m 22s\n",
      "\tTrain Loss: 0.018 | Train PPL:   1.018\n",
      "\t Val. Loss: 0.017 |  Val. PPL:   1.017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [43:13<18:37, 372.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Time: 6m 5s\n",
      "\tTrain Loss: 0.015 | Train PPL:   1.015\n",
      "\t Val. Loss: 0.017 |  Val. PPL:   1.017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [49:19<12:21, 370.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Time: 6m 6s\n",
      "\tTrain Loss: 0.014 | Train PPL:   1.014\n",
      "\t Val. Loss: 0.018 |  Val. PPL:   1.019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [55:25<06:08, 368.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Time: 6m 5s\n",
      "\tTrain Loss: 0.014 | Train PPL:   1.014\n",
      "\t Val. Loss: 0.013 |  Val. PPL:   1.013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [1:01:30<00:00, 369.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Time: 6m 4s\n",
      "\tTrain Loss: 0.013 | Train PPL:   1.013\n",
      "\t Val. Loss: 0.015 |  Val. PPL:   1.015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device( \"cuda\" if torch.cuda.is_available() else\"cpu\")\n",
    "n_epoch = 10\n",
    "kwargs = {\"matrix_embeddings\":weights, \n",
    "\"vocab_dim\": vocab_dim, \"output_dim\": output_dim, \"emb_dim\": emb_dim,\n",
    "\"hid_dim\": hidden_dim, \n",
    "\"n_layers\": n_layers, \n",
    "\"dropout\": dropout,\n",
    "\"batch_size\": batch_size}\n",
    "LAD_model = model.LogLSTM(weights, vocab_dim, output_dim, emb_dim, hidden_dim, n_layers, dropout,  batch_size).to(device)\n",
    "optimizer = optim.Adam(LAD_model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "try:\n",
    "    os.makedirs('model')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Training LSTM model\n",
    "clip = 1\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "for epoch in tqdm(range(n_epoch)):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_loss= model.train(LAD_model, train_iter, optimizer, criterion,  device)        \n",
    "\n",
    "    val_loss = model.evaluate(LAD_model, val_iter, criterion, device)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = model.epoch_time(start_time, end_time)\n",
    "    \n",
    "    if val_loss < best_test_loss:\n",
    "        best_test_loss = val_loss\n",
    "        torch.save({\n",
    "            'model_state_dict':LAD_model.state_dict(),\n",
    "            \"model_hyperparam\": kwargs,\n",
    "            \"W2V_conf\": {\n",
    "            'train_dict': train_dict, \n",
    "            'w2v_dict': w2v_dict,\n",
    "            \"WINDOW_SIZE\": WINDOW_SIZE,\n",
    "            \"STEP_SIZE\": STEP_SIZE\n",
    "            }\n",
    "        }, 'model/model_BGL.pt')\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. PPL: {math.exp(val_loss):7.3f}')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d79d43d0-011d-4f62-b8c8-f2f2cbcc05df",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "The model is evaluated using F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42b9c65a-b63d-4260-90ac-226ebef8edf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.98641   0.96198   0.97404      2867\n",
      "           1    0.92781   0.97359   0.95015      1439\n",
      "\n",
      "    accuracy                        0.96586      4306\n",
      "   macro avg    0.95711   0.96779   0.96210      4306\n",
      "weighted avg    0.96683   0.96586   0.96606      4306\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For evaluation the \n",
    "test_abnormal_ratio = model.ratio_abnormal_sequence(test_abnormal, WINDOW_SIZE, RATIO)\n",
    "test_ab_X, test_ab_X_key_label = test_abnormal_ratio['W2V_EventId'], test_abnormal_ratio['Key_label']\n",
    "test_n_X, test_n_X_key_label = test_normal['W2V_EventId'], test_normal['Key_label']\n",
    "test_ab_y = test_abnormal_ratio['Label']\n",
    "test_n_y = test_normal['Label']\n",
    "y, y_pre = model.model_precision(LAD_model, device, test_n_X.values.tolist()[:int(len(test_n_X.values.tolist())*(len(test_abnormal_ratio)/len(test_abnormal)))], \\\n",
    "                           test_ab_X.values.tolist())\n",
    "f1_acc = metrics.classification_report(y, y_pre, digits=5)\n",
    "print(f1_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7f25a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>EventId</th>\n",
       "      <th>Key_label</th>\n",
       "      <th>W2V_EventId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>0</td>\n",
       "      <td>[17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Label                                            EventId  \\\n",
       "10000      0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "10001      0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "10002      0  [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 1...   \n",
       "10003      0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "10004      0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                               Key_label  \\\n",
       "10000  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "10001  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "10002  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "10003  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "10004  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             W2V_EventId  \n",
       "10000  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "10001  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "10002  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...  \n",
       "10003  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "10004  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_normal.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0dcd6ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load trained model and parameters for inference\n",
    "\n",
    "# check_point  = torch.load('model/model_BGL.pt')\n",
    "\n",
    "# window_df = datatools.preprocess(df, check_point['W2V_conf']['WINDOW_SIZE'], check_point['W2V_conf']['STEP_SIZE'])\n",
    "\n",
    "\n",
    "# # convert to input vector\n",
    "# test_vector = datatools.test_vector(window_df, check_point['W2V_conf']['train_dict'], check_point['W2V_conf']['w2v_dict'])\n",
    "\n",
    "# # load LogLSTM model\n",
    "# trained_model_ = model.LogLSTM(**check_point['model_hyperparam']).to(device)\n",
    "# trained_model_.load_state_dict(check_point['model_state_dict'])\n",
    "\n",
    "# # predict label\n",
    "# _, y_pred = model.model_inference(trained_model_, device, test_vector['W2V_EventId'].values.tolist())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "980be0d1-edd9-4c02-a52f-4e75a67c4600",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this workflow, we show a pipeline for training sequence binary classifier to identify anomalous log sequence from set of generated log sequences. We used negative sampling to generate negative examples along normal logs for training the model. The model is evaluated on BGL dataset to identify alerts from non-alert messages. With an F1 score of 0.9 the model is able to identify true alerts from non-alert messages of test log samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96aa849-7069-4f0a-94cb-7afe6b4e4496",
   "metadata": {},
   "source": [
    "### Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0735f94a-1ccb-4a50-a754-3fbcd2b10b96",
   "metadata": {},
   "source": [
    "- https://arxiv.org/pdf/2202.04301.pdf\n",
    "- https://ieeexplore.ieee.org/document/9671642"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "f708a36acfaef0acf74ccd43dfb58100269bf08fb79032a1e0a6f35bd9856f51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
