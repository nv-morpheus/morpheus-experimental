{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ee655db-ac6b-49b8-9c7f-02d609e972a1",
   "metadata": {},
   "source": [
    "## Log sequence anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3deae48-58e4-4f40-9f0f-0414d2027118",
   "metadata": {},
   "source": [
    "### Content\n",
    "- Introduction\n",
    "- Dataset\n",
    "- Training and Evaluation\n",
    "- Conclusion\n",
    "- Reference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92ec9cca-7cc7-4674-b77f-47f31c8baec2",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Anomaly detection in sequential log data aims to identify sequences that deviate from the expected behavior\n",
    "or patterns. For example, software intensive systems often record runtime information by\n",
    "printing console logs. A large and complex system could produce a massive amount of logs, which can be used for troubleshooting\n",
    "purposes. The log messages can be modeled as an event sequence. It is critical to detect anomalous states\n",
    "in a timely manner to ensure the reliability the software system and mitigate the losses.\n",
    "\n",
    "Log data is usually unstructured text messages, which can help engineers understand the system’s internal\n",
    "status and facilitate monitoring, administering, and troubleshooting of the system Log messages can be parsed into log events,\n",
    "which are templates (constant part) of the messages. \n",
    " \n",
    "This usecase shows a workflow for identifying sequential anomalies from raw log sequence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "552e9437-779e-4e24-996c-4b27e82089d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import utils, datatools, model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import collections\n",
    "\n",
    "SEED = 91\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bb123e-e80b-417c-b41e-0fdd61e5e208",
   "metadata": {},
   "source": [
    "#### Dataset processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9eac0c7-4e4f-4816-8c53-09a9b7d0e138",
   "metadata": {},
   "source": [
    "The dataset for the example used from BlueGene/L Supercomputer System (BGL). BGL dataset contains 4,747,963 log messages that are collected\n",
    "from a [BlueGeme/L]('https://zenodo.org/record/3227177/files/BGL.tar.gz?download=1') supercomputer system at Lawrence Livermore National Labs. The log messages can be categorized into alert and not-alert messages. The log message is parsed using [`Drain`](https://github.com/logpai/logparser) parser into structured log format.\n",
    "\n",
    "For running this workflow we can use portion of parsed BGL dataset taken from  https://github.com/LogIntelligence/LogPPT. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f87177a-69bb-418f-a75c-e92c1ca8675a",
   "metadata": {},
   "source": [
    "#### Preprocessing log dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3597fe53-60a5-4b8b-923e-9a1da9918e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/LogIntelligence/LogPPT/master/logs/BGL/BGL_2k.log_structured.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f30b690-78a6-4697-a8e3-409f21fd3869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smaller dataset\n",
    "DATASET_NAME = 'https://raw.githubusercontent.com/LogIntelligence/LogPPT/master/logs/BGL/BGL_2k.log_structured.csv' #'BGL_2k'\n",
    "TRAIN_SIZE = 100 \n",
    "WINDOW_SIZE = 10\n",
    "STEP_SIZE = 20\n",
    "RATIO = 0.1\n",
    "\n",
    "# Full dataset \n",
    "# DATASET_NAME = 'dataset/bgl_1m.log_structured.csv'\n",
    "# TRAIN_SIZE = 10000 #00\n",
    "# WINDOW_SIZE = 100\n",
    "# STEP_SIZE = 20\n",
    "# RATIO = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e0926c",
   "metadata": {},
   "source": [
    "Create train and test dataset by transforming log dataset into embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a98e694-2dc5-4d1b-ab56-e26b9256ab71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: Dataset/bgl_1m.log_structured.csv\n",
      "Total logs in the dataset:  1000000\n",
      "training size 10000\n",
      "test normal size 27904\n",
      "test abnormal size 12096\n",
      "Number of all keys: 141\n",
      "Number of training keys: 106\n",
      "Word2Vec model: Word2Vec(vocab=107, size=8, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "train_normal, test_normal, test_abnormal, bigram, unique, weights, train_dict, w2v_dic = datatools.sliding_window(DATASET_NAME, WINDOW_SIZE, STEP_SIZE, TRAIN_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47a086ef-4fef-4284-9ee4-4e4b8c1cf478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 4)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_normal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27474c60-b43c-4bf0-bc1b-25580618a0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparmeters\n",
    "vocab_dim = len(train_dict)+1\n",
    "output_dim = 2\n",
    "emb_dim = 8\n",
    "hidden_dim = 128\n",
    "n_layers = 1\n",
    "dropout = 0.0\n",
    "batch_size = 32\n",
    "times = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "611b77cd-f891-43fe-9bee-ca2b08248192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(lst, label, dic):\n",
    "    df = pd.DataFrame()\n",
    "    df['EventId'] = lst\n",
    "    df['class_label'] = label\n",
    "    return datatools.str_key_to_w2v_index(df, dic)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40bfb858-d434-48dd-8081-899db08440f2",
   "metadata": {},
   "source": [
    "Generate negative samples and split into training data and validation data. Given a set of normal sequences, an anomalous sequences are generated via neative sampling. A binary sequence classifier is trained to classify the negative samples from the true positive samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd319ea5-b287-415b-8e1d-ea4e6ca83cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_samples = datatools.negative_sampling(train_normal, bigram, unique, times, vocab_dim)\n",
    "df_neg = get_dataframe(neg_samples, 1, w2v_dic)\n",
    "df_pos = get_dataframe(list(train_normal['EventId']), 0, w2v_dic)\n",
    "df_pos.columns = df_pos.columns.astype(str)\n",
    "df_train = pd.concat([df_pos, df_neg], ignore_index = True, axis=0)\n",
    "df_train.reset_index(drop = True)\n",
    "y = list(df_train.loc[:,'class_label'])\n",
    "X = list(df_train['W2V_EventId'])\n",
    "\n",
    "# split train, validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = torch.tensor(X_train,requires_grad=False).long()\n",
    "X_val = torch.tensor(X_val,requires_grad=False).long()\n",
    "y_train = torch.tensor(y_train).reshape(-1, 1).long()\n",
    "y_val = torch.tensor(y_val).reshape(-1, 1).long()\n",
    "train_iter = utils.get_iter(X_train, y_train, batch_size)\n",
    "val_iter = utils.get_iter(X_val, y_val, batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32e8c92b-d049-4194-8bdd-cf68c3b72655",
   "metadata": {},
   "source": [
    "### Training and Evaluation\n",
    "\n",
    "An LSTM model is trained using word2vector input genrated from both positive and negative examples with task of binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "661c5896-e3e4-4f28-ae39-035b1e5ccd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:47<07:09, 47.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 47s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.029 |  Val. PPL:   1.029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [01:35<06:21, 47.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 0m 47s\n",
      "\tTrain Loss: 0.019 | Train PPL:   1.020\n",
      "\t Val. Loss: 0.015 |  Val. PPL:   1.015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [02:22<05:32, 47.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Time: 0m 47s\n",
      "\tTrain Loss: 0.016 | Train PPL:   1.016\n",
      "\t Val. Loss: 0.014 |  Val. PPL:   1.014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [03:09<04:43, 47.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Time: 0m 47s\n",
      "\tTrain Loss: 0.012 | Train PPL:   1.012\n",
      "\t Val. Loss: 0.011 |  Val. PPL:   1.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [03:56<03:56, 47.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Time: 0m 47s\n",
      "\tTrain Loss: 0.010 | Train PPL:   1.010\n",
      "\t Val. Loss: 0.010 |  Val. PPL:   1.010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [04:45<03:10, 47.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Time: 0m 48s\n",
      "\tTrain Loss: 0.009 | Train PPL:   1.009\n",
      "\t Val. Loss: 0.010 |  Val. PPL:   1.010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [05:32<02:22, 47.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Time: 0m 47s\n",
      "\tTrain Loss: 0.007 | Train PPL:   1.007\n",
      "\t Val. Loss: 0.009 |  Val. PPL:   1.009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [06:20<01:35, 47.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Time: 0m 47s\n",
      "\tTrain Loss: 0.007 | Train PPL:   1.007\n",
      "\t Val. Loss: 0.010 |  Val. PPL:   1.010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [07:07<00:47, 47.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Time: 0m 47s\n",
      "\tTrain Loss: 0.006 | Train PPL:   1.006\n",
      "\t Val. Loss: 0.009 |  Val. PPL:   1.009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [07:55<00:00, 47.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Time: 0m 47s\n",
      "\tTrain Loss: 0.005 | Train PPL:   1.005\n",
      "\t Val. Loss: 0.010 |  Val. PPL:   1.010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device( \"cuda\" if torch.cuda.is_available() else\"cpu\")\n",
    "n_epoch = 10\n",
    "LAD_model = model.LogLSTM(weights, vocab_dim, output_dim, emb_dim, hidden_dim, n_layers, dropout, device, batch_size).to(device)\n",
    "optimizer = optim.Adam(LAD_model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "try:\n",
    "    os.makedirs('model')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Training LSTM model\n",
    "clip = 1\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "for epoch in tqdm(range(n_epoch)):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_loss= model.train(LAD_model, train_iter, optimizer, criterion, clip, epoch, device)        \n",
    "\n",
    "    val_loss = model.evaluate(LAD_model, val_iter, criterion, device)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = model.epoch_time(start_time, end_time)\n",
    "    \n",
    "    if val_loss < best_test_loss:\n",
    "        best_test_loss = val_loss\n",
    "        torch.save(LAD_model.state_dict(), 'model/model_BGL.pt')\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. PPL: {math.exp(val_loss):7.3f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d79d43d0-011d-4f62-b8c8-f2f2cbcc05df",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "The model is evaluated using F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42b9c65a-b63d-4260-90ac-226ebef8edf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.97895   0.98936       475\n",
      "           1    0.95370   1.00000   0.97630       206\n",
      "\n",
      "    accuracy                        0.98532       681\n",
      "   macro avg    0.97685   0.98947   0.98283       681\n",
      "weighted avg    0.98600   0.98532   0.98541       681\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For evaluation the \n",
    "test_abnormal_ratio = model.ratio_abnormal_sequence(test_abnormal, WINDOW_SIZE, RATIO)\n",
    "test_ab_X, test_ab_X_key_label = test_abnormal_ratio['W2V_EventId'], test_abnormal_ratio['Key_label']\n",
    "test_n_X, test_n_X_key_label = test_normal['W2V_EventId'], test_normal['Key_label']\n",
    "test_ab_y = test_abnormal_ratio['Label']\n",
    "test_n_y = test_normal['Label']\n",
    "y, y_pre = model.model_precision(LAD_model, device, test_n_X.values.tolist()[:int(len(test_n_X.values.tolist())*(len(test_abnormal_ratio)/len(test_abnormal)))], \\\n",
    "                           test_ab_X.values.tolist())\n",
    "f1_acc = metrics.classification_report(y, y_pre, digits=5)\n",
    "print(f1_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "742194be-cdf7-4fd7-a692-92d5895475b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pre"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "980be0d1-edd9-4c02-a52f-4e75a67c4600",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this workflow, we showed a pipeline for training sequence binary classifier to identify anomalous log sequence from normaly generated log sequences. We used negative sampling to generate negative exampels for training the model using only normal log sequence dataset. The model is evaluated on BGL dataset to identify alerts from non-alert messages for BGL dataset. With an F1 score of 0.9 the model is able to identify true alerts from non-alert messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96aa849-7069-4f0a-94cb-7afe6b4e4496",
   "metadata": {},
   "source": [
    "### Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0735f94a-1ccb-4a50-a754-3fbcd2b10b96",
   "metadata": {},
   "source": [
    "- https://arxiv.org/pdf/2202.04301.pdf\n",
    "- https://ieeexplore.ieee.org/document/9671642"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab4c2f4-8fc8-474a-a5d3-8522fd8e4ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "f708a36acfaef0acf74ccd43dfb58100269bf08fb79032a1e0a6f35bd9856f51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
