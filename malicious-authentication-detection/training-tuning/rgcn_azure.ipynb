{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6651de6-53a6-45b0-b41e-db295f2692e1",
   "metadata": {},
   "source": [
    "## Detection of Malicious Accounts on Azure-AD signon using Relational Graph Neural Network (RGCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef3ce1d-25e1-4e16-b98b-95e94ac5bd6e",
   "metadata": {},
   "source": [
    "### Content\n",
    "1. Introduction\n",
    "2. Dataset Loading & Processing\n",
    "3. Graph Construction\n",
    "4. Model Training\n",
    "5. Evaluation\n",
    "6. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0912b6-6364-4789-941a-b27ccee662ea",
   "metadata": {},
   "source": [
    "### 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e6eea4-4a9d-4d84-97ec-a22e5deb41cc",
   "metadata": {},
   "source": [
    "Azure active directory (Azure-AD) is an identity and access management service, that helps users to access external and internal resources such as Office365, SaaS applications. The Sign-in logs in Azure-AD log identifies who the user is, how the application is used for the access and the target accessed by the identity [1]. On a given time ùë°, a service ùë† is requested by user ùë¢ from device ùëë using authentication mechanism of ùëé to be either allowed or blocked.\n",
    "\n",
    "This workflow shows end-to-end pipeline for azure malicious sign-in malicious detection using relational graph neural network (RGCN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39f48fa1-796f-403e-8fe6-8db68401d80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import dgl.nn.pytorch as dglnn\n",
    "from utils import *\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from model import HeteroRGCN\n",
    "from tqdm import trange\n",
    "from data_processing import   build_azure_graph,  synthetic_azure\n",
    "\n",
    "from utils import get_metrics\n",
    "from model_training import init_loaders, train, evaluate, save_model, load_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9c71ef-6796-419e-989a-22c64643821e",
   "metadata": {},
   "source": [
    "\n",
    "### Dataset Loading and Processing\n",
    "Load dataset and set true fraud label for the testing and status flag for training. Define set of meta columns to exclude as training features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2775118d-684f-4548-b615-83ec196559c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_label = 'status_flag'\n",
    "result_dir = 'azure_result'\n",
    "\n",
    "\n",
    "meta_cols = ['day','appId', 'userId', 'ipAddress',\n",
    "            'fraud_label','appId_id','userId_id','ipAddress_id', 'auth_id', 'status_flag']\n",
    "\n",
    "train_data, test_data, train_idx, test_idx, labels, df  = synthetic_azure('../dataset/azure_synthetic/azure_ad_logs_sample_with_anomaly_train.json')\n",
    "\n",
    "fraud_labels = df['fraud_label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f404d0c8-35c0-47d8-b225-3e73959908a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The distribution of status flag for the train data is:\n",
      " 0    1245\n",
      "1     333\n",
      "Name: status_flag, dtype: int64\n",
      "The distribution of fraud for the test data is:\n",
      " 0.0    414\n",
      "Name: fraud_label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('The distribution of status flag for the train data is:\\n', train_data['status_flag'].value_counts())\n",
    "print('The distribution of fraud for the test data is:\\n', test_data['fraud_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5498692c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days in test set [239, 237, 236, 240, 241, 238]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Days in test set {test_data.day.unique().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b2d509a-38bc-4a06-9c96-f9c9042d4a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c09da9-efa3-472f-946d-1f104a0c0687",
   "metadata": {},
   "source": [
    "### Graph Construction\n",
    "\n",
    "Construct training and inference graph. Each graph is constructed using `train_data` and `test_data`, with graph schema defined in the `build_azure_graph` method. Create feature and compute weight for training proportion.\n",
    "\n",
    "![Azure-graph](graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f68861e3-5ec2-491e-80c8-096cfb9bb5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "g, _ = build_azure_graph(train_data, meta_cols)\n",
    "g_test, feature_tensors = build_azure_graph(df, meta_cols)\n",
    "n_nodes = sum([g.number_of_nodes(n_type) for n_type in g.ntypes])\n",
    "n_edges = sum([g.number_of_edges(e_type) for e_type in g.etypes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cee62f13-a417-4c4b-8fe2-2f098c1efb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "in_size, hidden_size, out_size, n_layers, embedding_size = feature_tensors.shape[1], 16, 2, 2, 8  # 2 16\n",
    "target_node = \"authentication\"\n",
    "hyperparameters = {\"in_size\":in_size, \"hidden_size\":hidden_size, \"out_size\":out_size,\n",
    "                   \"n_layers\":n_layers, \"embedding_size\":embedding_size, \"target_node\":target_node}\n",
    "labels = torch.LongTensor(labels).to(device)\n",
    "scale_pos_weight = train_data[status_label].sum() / train_data.shape[0]\n",
    "scale_pos_weight = torch.tensor(\n",
    "    [scale_pos_weight, 1-scale_pos_weight]).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a17609f-5082-44dc-b2df-ce13182e5831",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "#### RGCN Model\n",
    "\n",
    "At high level, the RGCN model takes graph ùê∫ of nodes features ùëã and learn an embedding of ùê∫ with a function ùëì:ùëâ‚ÜíùëÖùëë that map each node ùë£‚àà ùëâ to d-dimensional vector. The model is trained as a semi-supervised settings to classify each authentication as potential success or failure authentication from historical authentication logs. The embedding vector for ‚Äúauthentication‚Äù node is then used to score whether the authentication is benign or malicious. We compare the detection score of RGCN SoftMax layer along feeding the learned embedding to unsupervised anomaly detection algorithm isolation forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3633975-7b86-4f36-a2fe-695bd74580f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---- Data statistics ------\n",
      "            # Nodes: 2131\n",
      "            # Edges: 9468\n",
      "            # Features Shape: 1992\n",
      "            # Labeled Train samples: 1578\n",
      "            # Unlabeled Test samples: 414\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"\"\" ---- Data statistics ------\n",
    "            # Nodes: {}\n",
    "            # Edges: {}\n",
    "            # Features Shape: {}\n",
    "            # Labeled Train samples: {}\n",
    "            # Unlabeled Test samples: {}\"\"\".\n",
    "    format(\n",
    "        n_nodes, n_edges, feature_tensors.shape[0],\n",
    "        train_data.shape[0],\n",
    "        test_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05083534-711c-4497-bfdd-8dbdc9e40872",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "The model is trained using synthetic azure log data of 3 months for selected users and tested on 14 days of future days. Selected features are aggregated as OHE per day for individual users, app requested, and device activity logs and authentication behaviors. The `statusFailure` features is used for semi-supervised training of the model. It has binary values of either \"success\" or \"failure\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d542f55-3ec3-4e8a-96a5-3064e5565f13",
   "metadata": {},
   "source": [
    "Set dataloaders, define model and optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c9c968b-8b56-4631-96df-44d67d27443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "train_loader, val_loader, test_loader = init_loaders(\n",
    "    g, train_idx, test_idx=test_idx, val_idx=train_idx, g_test=g_test,\n",
    "    target_node=target_node)\n",
    "\n",
    "model = HeteroRGCN(g, in_size, hidden_size, out_size,\n",
    "                   n_layers, embedding_size, device=device, target=target_node).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.001, weight_decay=5e-4)\n",
    "loss_func = nn.CrossEntropyLoss(weight=scale_pos_weight.float())\n",
    "best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c030aba7-975b-479d-83ef-71269db11392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|‚ñã         | 2/30 [00:00<00:02, 13.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000/030 | Train Accuracy: 0.5986 | Train Loss: 1.3291\n",
      "Validation Accuracy: 0.6888 auc 0.7919\n",
      "Epoch 001/030 | Train Accuracy: 0.6869 | Train Loss: 1.3142\n",
      "Validation Accuracy: 0.7199 auc 0.7999\n",
      "Epoch 002/030 | Train Accuracy: 0.7336 | Train Loss: 1.3011\n",
      "Validation Accuracy: 0.7902 auc 0.8085\n",
      "Epoch 003/030 | Train Accuracy: 0.7924 | Train Loss: 1.2884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà        | 6/30 [00:00<00:01, 14.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8099 auc 0.8162\n",
      "Epoch 004/030 | Train Accuracy: 0.8183 | Train Loss: 1.2758\n",
      "Validation Accuracy: 0.8346 auc 0.8251\n",
      "Epoch 005/030 | Train Accuracy: 0.8287 | Train Loss: 1.2634\n",
      "Validation Accuracy: 0.8397 auc 0.8313\n",
      "Epoch 006/030 | Train Accuracy: 0.8339 | Train Loss: 1.2510\n",
      "Validation Accuracy: 0.8549 auc 0.8383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 10/30 [00:00<00:01, 14.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 007/030 | Train Accuracy: 0.8426 | Train Loss: 1.2386\n",
      "Validation Accuracy: 0.8657 auc 0.8446\n",
      "Epoch 008/030 | Train Accuracy: 0.8426 | Train Loss: 1.2262\n",
      "Validation Accuracy: 0.8669 auc 0.8481\n",
      "Epoch 009/030 | Train Accuracy: 0.8633 | Train Loss: 1.2138\n",
      "Validation Accuracy: 0.8904 auc 0.8504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 12/30 [00:00<00:01, 14.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010/030 | Train Accuracy: 0.8806 | Train Loss: 1.2013\n",
      "Validation Accuracy: 0.8904 auc 0.8541\n",
      "Epoch 011/030 | Train Accuracy: 0.8806 | Train Loss: 1.1887\n",
      "Validation Accuracy: 0.8904 auc 0.8564\n",
      "Epoch 012/030 | Train Accuracy: 0.8806 | Train Loss: 1.1760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 14/30 [00:01<00:01, 12.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8916 auc 0.8573\n",
      "Epoch 013/030 | Train Accuracy: 0.8806 | Train Loss: 1.1635\n",
      "Validation Accuracy: 0.8916 auc 0.8582\n",
      "Epoch 014/030 | Train Accuracy: 0.8806 | Train Loss: 1.1508\n",
      "Validation Accuracy: 0.8885 auc 0.8574\n",
      "Epoch 015/030 | Train Accuracy: 0.8806 | Train Loss: 1.1381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 18/30 [00:01<00:00, 13.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8853 auc 0.8586\n",
      "Epoch 016/030 | Train Accuracy: 0.8633 | Train Loss: 1.1253\n",
      "Validation Accuracy: 0.8853 auc 0.8592\n",
      "Epoch 017/030 | Train Accuracy: 0.8443 | Train Loss: 1.1125\n",
      "Validation Accuracy: 0.8783 auc 0.8601\n",
      "Epoch 018/030 | Train Accuracy: 0.8443 | Train Loss: 1.0997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 20/30 [00:01<00:00, 13.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8783 auc 0.8614\n",
      "Epoch 019/030 | Train Accuracy: 0.8443 | Train Loss: 1.0870\n",
      "Validation Accuracy: 0.8771 auc 0.8626\n",
      "Epoch 020/030 | Train Accuracy: 0.8443 | Train Loss: 1.0744\n",
      "Validation Accuracy: 0.8771 auc 0.8636\n",
      "Epoch 021/030 | Train Accuracy: 0.8443 | Train Loss: 1.0619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 24/30 [00:01<00:00, 13.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8771 auc 0.8652\n",
      "Epoch 022/030 | Train Accuracy: 0.8443 | Train Loss: 1.0496\n",
      "Validation Accuracy: 0.8771 auc 0.8665\n",
      "Epoch 023/030 | Train Accuracy: 0.8443 | Train Loss: 1.0375\n",
      "Validation Accuracy: 0.8771 auc 0.8675\n",
      "Epoch 024/030 | Train Accuracy: 0.8408 | Train Loss: 1.0256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 26/30 [00:01<00:00, 13.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8745 auc 0.8689\n",
      "Epoch 025/030 | Train Accuracy: 0.8408 | Train Loss: 1.0140\n",
      "Validation Accuracy: 0.8752 auc 0.8698\n",
      "Epoch 026/030 | Train Accuracy: 0.8374 | Train Loss: 1.0027\n",
      "Validation Accuracy: 0.8701 auc 0.8708\n",
      "Epoch 027/030 | Train Accuracy: 0.8374 | Train Loss: 0.9918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:02<00:00, 13.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8701 auc 0.8725\n",
      "Epoch 028/030 | Train Accuracy: 0.8374 | Train Loss: 0.9812\n",
      "Validation Accuracy: 0.8682 auc 0.8743\n",
      "Epoch 029/030 | Train Accuracy: 0.8356 | Train Loss: 0.9711\n",
      "Validation Accuracy: 0.8676 auc 0.8764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_auc = 0\n",
    "for epoch in trange(epochs):\n",
    "    train_acc, loss = train(\n",
    "        model, loss_func, train_loader, labels, optimizer, feature_tensors,\n",
    "        target_node=target_node, device=device)\n",
    "    print(\"Epoch {:03d}/{:03d} | Train Accuracy: {:.4f} | Train Loss: {:.4f}\".format(\n",
    "        epoch, epochs, train_acc, loss))\n",
    "\n",
    "    val_logits, val_seed, _ = evaluate(\n",
    "        model, val_loader, feature_tensors, target_node, device=device)\n",
    "    val_accuracy = accuracy_score(\n",
    "        val_logits.argmax(1),\n",
    "        labels.long()[val_seed].cpu()).item()\n",
    "    val_auc = roc_auc_score(labels.long()[val_seed].cpu().numpy(),\n",
    "                            val_logits[:, 1].numpy(),)\n",
    "    print(\n",
    "        \"Validation Accuracy: {:.4f} auc {:.4f}\".format(\n",
    "            val_accuracy, val_auc))\n",
    "        \n",
    "best_model = model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60f91fe-8a76-4281-aed6-38e08b645f55",
   "metadata": {},
   "source": [
    "Save model, hyperparameters and graph structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "163b6f46-4f27-4fa7-82b5-9ca3cf9338d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "save_model(g, model, hyperparameters, \"../modeldir/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b56c3508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model for inference\n",
    "model_new, g_ = load_model(\"../modeldir/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d94ce3-f08e-43d6-8653-8a7972616033",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "For evaluation and comparing we need to create the \"authentication\" embedding from the training and test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4c938f8-7945-4f62-8ac7-9ee94fa4a02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create training & test embedding\n",
    "train_logits, train_seeds, train_embedding = evaluate(\n",
    "    best_model, train_loader, feature_tensors, target_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02c323c7-07f6-432c-8769-350dce555b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logits, test_seeds, test_embedding = evaluate(\n",
    "    best_model, test_loader, feature_tensors, target_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e149ed5-1b00-4ad7-8ec1-989e3e5eb1b6",
   "metadata": {},
   "source": [
    "The `train_logit` and `test_logits` are the output of the RGCN classification layer. We can evaluate the model using `test_logits` along evaluation label. The `train_embedding` and `test_embedding` dataframe consists of an embedding of the authentication. These embedding\n",
    "can be used to train & evaluate for other baseline models, such as XGBoost or unsupervised model (Isoltion Forest).\n",
    "\n",
    "Using these embedding we compare the performance of RGCN based model against baseline algorithm trained & tested on the authentication embedding on internal dataset of more than 45k events for training and 11k events for testing. The result is shown in the following figure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ef623e-8ba2-46c2-af53-04d018ba5e42",
   "metadata": {},
   "source": [
    "![model comparison](model_comp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c6c402",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cafe0fb",
   "metadata": {},
   "source": [
    "In this workflow we showed end-to-end worklfow for malicious azure detection using RGCN.\n",
    "we explore methods of heterogeneous graph embedding for malicious sign-on detection of azure logs. This work adds two main contributions. First, adapting log authentication as GNN, allows us to learn a richer embedding of authentication on both structural and individual entities involved without much hand-crafted feature learning. Second, by modeling every ‚Äúauthentication‚Äù as a target node, the model avoids the challenge of depending on modeling temporal historical user login information. The experimental result on internal data shows, RGCN prediction scores on authentication nodes have a promising result on overall detection performance and are better than the baseline isolation forest & XGBoost algorithm applied to the learned embedding vector of authentication nodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb226b8",
   "metadata": {},
   "source": [
    "### Reference\n",
    "1. https://docs.microsoft.com/en-us/azure/active-directory/reports-monitoring/concept-sign-ins\n",
    "2. Liu, Ziqi, et al. ‚ÄúHeterogeneous Graph Neural Networks for Malicious Account Detection.‚Äù arXiv [cs.LG], 27 Feb. 2020, https://doi.org/10.1145/3269206.3272010. arXiv.\n",
    "3. Lv, Mingqi, et al. ‚ÄúA Heterogeneous Graph Learning Model for Cyber-Attack Detection.‚Äù arXiv [cs.CR], 16 Dec. 2021, http://arxiv.org/abs/2112.08986. arXiv.\n",
    "4. Schlichtkrull, Michael, et al. \"Modeling relational data with graph convolutional networks.\" European semantic web conference. Springer, Cham, 2018\n",
    "5. Rao, Susie Xi, et al. \"xFraud: explainable fraud transaction detection.\" Proceedings of the VLDB Endowment 3 (2021)\n",
    "6. Powell, Brian A. \"Detecting malicious logins as graph anomalies.\" Journal of Information Security and Applications 54 (2020): 102557"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821efdd3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "f708a36acfaef0acf74ccd43dfb58100269bf08fb79032a1e0a6f35bd9856f51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
