{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook loads a pre-trained XGBoost model and runs inference on raw data\n",
    "__NOTE__: This XGBoost model does not leverage embeddings from the GNN (GraphSAGE) model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals\n",
    "* Outline the steps to transform new raw data before feeding it into the model.\n",
    "* Simulate the use of the trained model on new data during inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import xgboost as xgb\n",
    "from cuml.metrics import confusion_matrix\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Path to the pre-trained XGBoost model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_base_path = '../data/TabFormer'\n",
    "model_root_dir = os.path.join(dataset_base_path, 'models')\n",
    "model_file_name = 'xgboost_model.json'\n",
    "xgb_model_path = os.path.join(model_root_dir, model_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "#### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load xgboost model for node classification\n",
    "loaded_bst = xgb.Booster()\n",
    "loaded_bst.load_model(xgb_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load column names and other global variables saved during the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSON file\n",
    "with open(os.path.join(dataset_base_path, 'variables.json'), 'r') as json_file:\n",
    "    column_names = json.load(json_file)\n",
    "\n",
    "# Repopulate the variables in the global namespace\n",
    "globals().update(column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### Evaluate the XGBoost model on untransformed test data (saved in the preprocessing notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read untransformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('future.no_silent_downcasting', True)    \n",
    "path_to_untransformed_data = os.path.join(dataset_base_path, 'xgb', 'untransformed_test.csv')\n",
    "untransformed_df = pd.read_csv(path_to_untransformed_data)\n",
    "untransformed_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data transformer and transform the data using the loaded transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dataset_base_path, 'preprocessor.pkl'),'rb') as f:\n",
    "    loaded_transformer = pickle.load(f)\n",
    "    transformed_data = loaded_transformer.transform(\n",
    "        untransformed_df.loc[:, untransformed_df.columns[:-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate the model on the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor columns used for training\n",
    "numerical_predictors = [COL_AMOUNT]\n",
    "nominal_predictors = [COL_ERROR, COL_CARD, COL_CHIP, COL_CITY, COL_ZIP, COL_MCC, COL_MERCHANT]\n",
    "\n",
    "predictor_columns = numerical_predictors + nominal_predictors\n",
    "predictor_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed column names\n",
    "columns_of_transformed_data = list(\n",
    "    map(lambda name: name.split('__')[1],\n",
    "        list(loaded_transformer.get_feature_names_out(predictor_columns))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features (X) and target (y)\n",
    "X = pd.DataFrame(\n",
    "    transformed_data, columns=columns_of_transformed_data)\n",
    "\n",
    "y = untransformed_df[untransformed_df.columns[-1]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "\n",
    "y_pred_prob = loaded_bst.predict(xgb.DMatrix(data=X, label=y))\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute metrics to evaluate the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_mat = confusion_matrix(y, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(conf_mat)\n",
    "\n",
    "# ROC AUC Score\n",
    "r_auc = roc_auc_score(y, y_pred_prob)\n",
    "print(f'ROC AUC Score: {r_auc:.4f}')\n",
    "\n",
    "# y = cupy.asnumpy(y)\n",
    "# Precision\n",
    "precision = precision_score(y, y_pred)\n",
    "print(f'Precision: {precision:.4f}')\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(y, y_pred)\n",
    "print(f'Recall: {recall:.4f}')\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y, y_pred)\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Prediction on raw inputs\n",
    "* The purpose is to demonstrate the use of the model during inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read example raw inputs\n",
    "\n",
    "raw_file_path = os.path.join(dataset_base_path, 'xgb', 'example_transactions.csv')\n",
    "data = pd.read_csv(raw_file_path)\n",
    "data = data[data.columns[:-1]]\n",
    "original_data = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename columns before the data is fed into the pre-fitted data transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = data.rename(columns={\n",
    "    \"Merchant Name\": COL_MERCHANT,\n",
    "    \"Merchant State\": COL_STATE,\n",
    "    \"Merchant City\": COL_CITY,\n",
    "    \"Errors?\": COL_ERROR,\n",
    "    \"Use Chip\": COL_CHIP\n",
    "    },\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle unknown values as was done for the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNKNOWN_STRING_MARKER = 'XX'\n",
    "UNKNOWN_ZIP_CODE = 0\n",
    "\n",
    "data[COL_STATE] = data[COL_STATE].fillna(UNKNOWN_STRING_MARKER)\n",
    "data[COL_ERROR] = data[COL_ERROR].fillna(UNKNOWN_STRING_MARKER)\n",
    "data[COL_ZIP] = data[COL_ZIP].fillna(UNKNOWN_ZIP_CODE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert column type and remove \"$\" and \",\" as was done for the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data[COL_AMOUNT] = data[COL_AMOUNT].str.replace(\"$\",\"\").astype(\"float\")\n",
    "data[COL_STATE] = data[COL_STATE].astype('str')\n",
    "data[COL_MERCHANT] = data[COL_MERCHANT].astype('str')\n",
    "data[COL_ERROR] = data[COL_ERROR].str.replace(\",\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine User and Card to generate unique numbers as was done for the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data[COL_CARD] = data[COL_USER] * MAX_NR_CARDS_PER_USER  + data[COL_CARD]\n",
    "data[COL_CARD] = data[COL_CARD].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check if the transactions have unknown users or merchants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the known merchants and (users, cards), i.e. the merchants and (users, cards) that are in training data\n",
    "known_merchants = set()\n",
    "known_cards = set()\n",
    "\n",
    "for enc in  loaded_transformer.named_transformers_['binary'].named_steps['binary'].ordinal_encoder.mapping:\n",
    "    if enc['col'] == COL_MERCHANT:\n",
    "        known_merchants = set(enc['mapping'].keys())\n",
    "    if enc['col'] == COL_CARD:\n",
    "        known_cards = set(enc['mapping'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is user, card already known\n",
    "data['Is_card_known'] = data[COL_CARD].map(lambda c: c in known_cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is merchant already known\n",
    "data['Is_merchant_known'] = data[COL_MERCHANT].map(lambda m: m in known_merchants )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the same set of predictor columns as used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_predictors = [COL_AMOUNT]\n",
    "nominal_predictors = [COL_ERROR, COL_CARD, COL_CHIP, COL_CITY, COL_ZIP, COL_MCC, COL_MERCHANT]\n",
    "\n",
    "predictor_columns = numerical_predictors + nominal_predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the data transformer and transform the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dataset_base_path, 'preprocessor.pkl'),'rb') as f:\n",
    "    loaded_transformer = pickle.load(f)\n",
    "    transformed_data = loaded_transformer.transform(data[predictor_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = pd.DataFrame(\n",
    "    transformed_data, columns=columns_of_transformed_data)\n",
    "\n",
    "# Predict transactions\n",
    "pred_probs = loaded_bst.predict(xgb.DMatrix(X))\n",
    "pred_labels = (pred_probs >= 0.5).astype(int)\n",
    "\n",
    "# Name of the target column\n",
    "target_col_name = 'Is Fraud?'\n",
    "\n",
    "data[target_col_name] = pred_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If the transactions have unknown (user, card) or merchant, mark it as fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data[target_col_name] = data.apply(\n",
    "    lambda row: \n",
    "    (row[target_col_name] == 1) or (row['Is_card_known'] == False) or (row['Is_merchant_known'] == False), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label the raw data as Fraud or Non-Fraud, based on prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Change 0 to No (non-Fraud) and 1 to Yes (Fraud)\n",
    "binary_to_text = { False: 'No', True : 'Yes'}\n",
    "data[target_col_name] = data[target_col_name].map(binary_to_text).astype('str')\n",
    "original_data[target_col_name] = data[target_col_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transactions with predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copyright and License\n",
    "<hr/>\n",
    "Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.\n",
    "\n",
    "<br/>\n",
    "\n",
    " Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    " you may not use this file except in compliance with the License.\n",
    " You may obtain a copy of the License at\n",
    " \n",
    " http://www.apache.org/licenses/LICENSE-2.0\n",
    " \n",
    " Unless required by applicable law or agreed to in writing, software\n",
    " distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    " WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    " See the License for the specific language governing permissions and\n",
    " limitations under the License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
