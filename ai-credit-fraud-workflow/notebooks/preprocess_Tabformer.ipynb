{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c6a5b09-a601-47c6-989f-5efb42d7f4f8",
   "metadata": {},
   "source": [
    "# Credit Card Transaction Data Cleanup and Prep \n",
    "\n",
    "This notebook shows the steps for cleanup and preparing the credit card transaction data for follow on GNN training with GraphSAGE.\n",
    "\n",
    "### The dataset:\n",
    " * IBM TabFormer: https://github.com/IBM/TabFormer\n",
    " * Released under an Apache 2.0 license\n",
    "\n",
    "Contains 24M records with 15 fields, one field being the \"is fraud\" label which we use for training.\n",
    "\n",
    "### Goals\n",
    "The goal is to:\n",
    " * Cleanup the data\n",
    "   * Make field names just single word\n",
    "     * while field names are not used within the GNN, it makes accessing fields easier during cleanup  \n",
    "   * Encode categorical fields\n",
    "     * use one-hot encoding for fields with less than 8 categories\n",
    "     * use binary encoding for fields with more than 8 categories\n",
    "   * Create a continuous node index across users, merchants, and transactions\n",
    "     * having node ID start at zero and then be contiguous is critical for creation of Compressed Sparse Row (CSR) formatted data without wasting memory.\n",
    " * Produce:\n",
    "   * For XGBoost:\n",
    "     * Training   - all data before 2018\n",
    "     * Validation - all data during 2018\n",
    "     * Test.      - all data after 2018\n",
    "   * For GNN\n",
    "     * Training Data \n",
    "       * Edge List \n",
    "       * Feature data\n",
    "   * Test set - all data after 2018\n",
    "\n",
    "\n",
    "\n",
    "### Graph formation\n",
    "Given that we are limited to just the data in the transaction file, the ideal model would be to have a bipartite graph of Users to Merchants where the edges represent the credit card transaction and then perform Link Classification on the Edges to identify fraud. Unfortunately the current version of cuGraph does not support GNN Link Prediction. That limitation will be lifted over the next few release at which time this code will be updated. Luckily, there is precedence for viewing transactions as nodes and then doing node classification using the popular GraphSAGE GNN. That is the approach this code takes. The produced graph will be a tri-partite graph where each transaction is represented as a node.\n",
    "\n",
    "<img src=\"../img/3-partite.jpg\" width=\"35%\"/>\n",
    "\n",
    "\n",
    "### Features\n",
    "For the XGBoost approach, there is no need to generate empty features for the Merchants. However, for GNN processing, every node needs to have the same set of feature data. Therefore, we need to generate empty features for the User and Merchant nodes. \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795bdece",
   "metadata": {},
   "source": [
    "#### Import the necessary libraries.  In this case will be use cuDF and perform most of the data prep in GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b6b2bc6-a206-42c5-aae9-590672b3a202",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import cudf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as ss\n",
    "from category_encoders import BinaryEncoder\n",
    "from scipy.stats import pointbiserialr\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81db641b",
   "metadata": {},
   "source": [
    "-------\n",
    "#### Define some arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "016964ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether the graph is undirected\n",
    "make_undirected = True\n",
    "\n",
    "# Whether to spread features across Users and Merchants nodes\n",
    "spread_features = False\n",
    "\n",
    "# Whether we should under-sample majority class (i.e. non-fraud transactions)\n",
    "under_sample = True\n",
    "\n",
    "# Ration of fraud and non-fraud transactions in case we under-sample the majority class\n",
    "fraud_ratio = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "656e6aee-038a-4b58-9296-993e06defb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabformer_base_path = '../data/TabFormer'\n",
    "tabformer_raw_file_path = os.path.join(tabformer_base_path, 'raw', 'card_transaction.v1.csv')\n",
    "tabformer_xgb = os.path.join(tabformer_base_path, 'xgb')\n",
    "tabformer_gnn = os.path.join(tabformer_base_path, 'gnn')\n",
    "\n",
    "if not os.path.exists(tabformer_xgb):\n",
    "    os.makedirs(tabformer_xgb)\n",
    "if not os.path.exists(tabformer_gnn):\n",
    "    os.makedirs(tabformer_gnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fe43fe",
   "metadata": {},
   "source": [
    "--------\n",
    "#### Load and understand the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb41e6ea-1e9f-4f14-99a4-d6d3df092a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "data = cudf.read_csv(tabformer_raw_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9a9ab4-4240-4824-997b-8bfdb640381c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional - take a look at the data \n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66495f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73172495",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "* Ordinal categorical fields - 'Year', 'Month', 'Day'\n",
    "* Nominal categorical fields - 'User', 'Card', 'Merchant Name', 'Merchant City', 'Merchant State', 'Zip', 'MCC', 'Errors?'\n",
    "* Target label - 'Is Fraud?'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f285adae",
   "metadata": {},
   "source": [
    "#### Check if are there Null values in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f58262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which fields are missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f20eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check percentage of missing values\n",
    "100*data.isnull().sum()/len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805d62ba",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "* For many transactions 'Merchant State' and 'Zip' are missing, but it's good that all of the transactions have 'Merchant City' specified. \n",
    "* Over 98% of the transactions are missing data for 'Errors?' fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33487e74",
   "metadata": {},
   "source": [
    "##### Save a few transactions before any operations on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8c188c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a few raw transactions for model's inference notebook\n",
    "out_path = os.path.join(tabformer_xgb, 'example_transactions.csv')\n",
    "data.tail(10).to_pandas().to_csv(out_path, header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57513227",
   "metadata": {},
   "source": [
    "#### Let's rename the column names to single words and use variables for column names to make access easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d35f7230",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "COL_USER = 'User'\n",
    "COL_CARD = 'Card'\n",
    "COL_AMOUNT = 'Amount'\n",
    "COL_MCC = 'MCC'\n",
    "COL_TIME = 'Time'\n",
    "COL_DAY = 'Day'\n",
    "COL_MONTH = 'Month'\n",
    "COL_YEAR = 'Year'\n",
    "\n",
    "COL_MERCHANT = 'Merchant'\n",
    "COL_STATE ='State'\n",
    "COL_CITY ='City'\n",
    "COL_ZIP = 'Zip'\n",
    "COL_ERROR = 'Errors'\n",
    "COL_CHIP = 'Chip'\n",
    "COL_FRAUD = 'Fraud'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90aa3fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_ = data.rename(columns={\n",
    "    \"Merchant Name\": COL_MERCHANT,\n",
    "    \"Merchant State\": COL_STATE,\n",
    "    \"Merchant City\": COL_CITY,\n",
    "    \"Errors?\": COL_ERROR,\n",
    "    \"Use Chip\": COL_CHIP,\n",
    "    \"Is Fraud?\": COL_FRAUD\n",
    "    },\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee33e39b",
   "metadata": {},
   "source": [
    "#### Handle missing values\n",
    "* Zip codes are numeral, replace missing zip codes by 0\n",
    "* State and Error are string, replace missing values by marker 'XX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79e24ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNKNOWN_STRING_MARKER = 'XX'\n",
    "UNKNOWN_ZIP_CODE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b774e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that 'XX' doesn't exist in State and Error field before we replace missing values by 'XX'\n",
    "assert(UNKNOWN_STRING_MARKER not in set(data[COL_STATE].unique().to_pandas()))\n",
    "assert(UNKNOWN_STRING_MARKER not in set(data[COL_ERROR].unique().to_pandas()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7964564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that 0 or 0.0 doesn't exist in Zip field before we replace missing values by 0\n",
    "assert(float(0) not in set(data[COL_ZIP].unique().to_pandas()))\n",
    "assert(0 not in set(data[COL_ZIP].unique().to_pandas()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1baca88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values with markers\n",
    "data[COL_STATE] = data[COL_STATE].fillna(UNKNOWN_STRING_MARKER)\n",
    "data[COL_ERROR] = data[COL_ERROR].fillna(UNKNOWN_STRING_MARKER)\n",
    "data[COL_ZIP] = data[COL_ZIP].fillna(UNKNOWN_ZIP_CODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07cf40c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There shouldn't be any missing values in the data now.\n",
    "assert(data.isnull().sum().sum() == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f027291-5d0b-4917-ada0-a0dbe6b80f9b",
   "metadata": {},
   "source": [
    "### Clean up the Amount field\n",
    "* Drop the \"$\" from the Amount field and then convert from string to float\n",
    "* Look into spread of Amount and choose right scaler for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ffe11c2-5e6d-4fac-8b42-27efb02afa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the \"$\" from the Amount field and then convert from string to float \n",
    "data[COL_AMOUNT] = data[COL_AMOUNT].str.replace(\"$\",\"\").astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bd4966",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[COL_AMOUNT].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab82a9f9",
   "metadata": {},
   "source": [
    "#### Let's look into how the Amount differ between fraud and non-fraud transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31679a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraud transactions\n",
    "data[COL_AMOUNT][data[COL_FRAUD]=='Yes'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f49ff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-fraud transactions\n",
    "data[COL_AMOUNT][data[COL_FRAUD]=='No'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a190bb9e",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "* 25th percentile = 9.2\n",
    "* 75th percentile =  65\n",
    "* Median is around 30 and the mean is around 43 whereas the max value is over 1200 and min value is -500\n",
    "* Average amount in Fraud transactions > 2x the average amount in Non-Fraud transactions\n",
    "\n",
    "We need to scale the data, and RobustScaler could be a good choice for it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96a9ae1-1dcf-4480-a808-3afa913cb292",
   "metadata": {},
   "source": [
    "#### Now the \"Fraud\" field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b6c719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many different categories are there in the COL_FRAUD column?\n",
    "# The hope is that there are only two categories, 'Yes' and 'No'\n",
    "data[COL_FRAUD].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5004040",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[COL_FRAUD].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d498e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "100 * data[COL_FRAUD].value_counts()/len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f13282",
   "metadata": {},
   "source": [
    "#### Change the 'Fraud' values to be integer where\n",
    "  * 1 == Fraud\n",
    "  * 0 == Non-fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa31c844",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_to_binary = {'No': 0, 'Yes': 1}\n",
    "data[COL_FRAUD] = data[COL_FRAUD].map(fraud_to_binary).astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7527510d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[COL_FRAUD].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f56d5a-f135-4af2-ba13-b926f66a045f",
   "metadata": {},
   "source": [
    "#### The 'City', 'State', and 'Zip' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dff40cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# City\n",
    "data[COL_CITY].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdf46ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State\n",
    "data[COL_STATE].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36297321-fb9b-48c6-afce-f083834eea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip\n",
    "data[COL_ZIP].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab51419d-c051-489b-af63-935248c133d0",
   "metadata": {},
   "source": [
    "#### The 'Chip' column\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae85372-0f22-4850-bfa4-b8513b742663",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[COL_CHIP].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22939e0f-bae0-4af3-aa3b-1b79974c0697",
   "metadata": {},
   "source": [
    "#### The 'Error' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b877a558-4306-49f1-aa75-ba535be4470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[COL_ERROR].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa6a67c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove ',' in error descriptions\n",
    "data[COL_ERROR] = data[COL_ERROR].str.replace(\",\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ca593a",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "We can one hot or binary encode columns with fewer categories and binary/hash encode columns with more than 8 categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50933790-780c-43cc-833d-c7ad16acbde3",
   "metadata": {},
   "source": [
    "#### Time\n",
    "Time is captured as hour:minute.\n",
    "\n",
    "We are converting the time to just be the number of minutes.\n",
    "\n",
    "time = (hour * 60) + minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97c5a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[COL_TIME].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1df15290-f60f-416d-81a4-437ff45b6d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the time column into hours and minutes and then cast to int32\n",
    "T = data[COL_TIME].str.split(':', expand=True)\n",
    "T[0] = T[0].astype('int32')\n",
    "T[1] = T[1].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15d77736-53dd-4af6-a475-9ad812f84731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the 'Time' column with the new columns\n",
    "data[COL_TIME] = (T[0] * 60 ) + T[1]\n",
    "data[COL_TIME] = data[COL_TIME].astype(\"int32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d51b6840-2912-4ecb-9998-7adc680f9d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete temporary DataFrame\n",
    "del(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d41134",
   "metadata": {},
   "source": [
    "#### Merchant column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac83d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[COL_MERCHANT] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f79e111",
   "metadata": {},
   "source": [
    "#### Convert the column to str type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0348c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[COL_MERCHANT] = data[COL_MERCHANT].astype('str')\n",
    "\n",
    "# TOver 100,000 merchants\n",
    "data[COL_MERCHANT].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b4daee",
   "metadata": {},
   "source": [
    "#### The Card column\n",
    "* \"Card 0\" for User 1 is different from \"Card 0\" for User 2.\n",
    "* Combine User and Card in a way such that (User, Card) combination is unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2abade",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[COL_CARD].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "068a05b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_nr_cards_per_user = len(data[COL_CARD].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a64bb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine User and Card to generate unique numbers\n",
    "data[COL_CARD] = data[COL_USER] * len(data[COL_CARD].unique())  + data[COL_CARD]\n",
    "data[COL_CARD] = data[COL_CARD].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a815bc9",
   "metadata": {},
   "source": [
    "#### Define function to compute correlation of different categorical fields with target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cfaa31fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/Cram%C3%A9r's_V\n",
    "\n",
    "def cramers_v(x, y):\n",
    "    confusion_matrix = cudf.crosstab(x, y).to_numpy()\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    r, k = confusion_matrix.shape\n",
    "    return np.sqrt(chi2 / (n * (min(k-1, r-1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa39773",
   "metadata": {},
   "source": [
    "##### Compute correlation of different fields with target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5497f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_factor = 1\n",
    "columns_to_compute_corr =  [COL_CARD, COL_CHIP, COL_ERROR, COL_STATE, COL_CITY, COL_ZIP, COL_MCC, COL_MERCHANT, COL_USER, COL_DAY, COL_MONTH, COL_YEAR]\n",
    "for c1 in columns_to_compute_corr:\n",
    "    for c2 in [COL_FRAUD]:\n",
    "        coff =  100 * cramers_v(data[c1][::sparse_factor], data[c2][::sparse_factor])\n",
    "        print('Correlation ({}, {}) = {:6.2f}%'.format(c1, c2, coff))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbc4636",
   "metadata": {},
   "source": [
    "### Correlation of target with numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a624f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/Point-biserial_correlation_coefficient\n",
    "# Use Point-biserial correlation coefficient(rpb) to check if the numerical columns are important to predict if a transaction is fraud\n",
    "\n",
    "\n",
    "for col in [COL_TIME, COL_AMOUNT]:\n",
    "    r_pb, p_value = pointbiserialr(data[COL_FRAUD].to_pandas(), data[col].to_pandas())\n",
    "    print('r_pb ({}) = {:3.2f} with p_value {:3.2f}'.format(col,  r_pb, p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041e3c50",
   "metadata": {},
   "source": [
    "### Findings\n",
    "* Clearly, Time is not an important predictor\n",
    "* Amount has 3% correlation with target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92d58ef",
   "metadata": {},
   "source": [
    "#### Based on correlation, select a set of columns (aka fields) to predict whether a transaction is fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a00a7813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the cross correlation of Fraud with Day, Month, Year is significantly lower,\n",
    "# we can skip them for now and add these features later.\n",
    "\n",
    "numerical_predictors = [COL_AMOUNT]\n",
    "nominal_predictors = [COL_ERROR, COL_CARD, COL_CHIP, COL_CITY, COL_ZIP, COL_MCC, COL_MERCHANT]\n",
    "\n",
    "predictor_columns = numerical_predictors + nominal_predictors\n",
    "\n",
    "target_column = [COL_FRAUD]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9341f5e0",
   "metadata": {},
   "source": [
    "#### Remove duplicates non-fraud data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "428c4ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates data points\n",
    "fraud_data = data[data[COL_FRAUD] == 1]\n",
    "data = data[data[COL_FRAUD] == 0]\n",
    "data = data.drop_duplicates(subset=nominal_predictors)\n",
    "data = cudf.concat([data, fraud_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a8bbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of fraud and non-fraud cases\n",
    "100*data[COL_FRAUD].value_counts()/len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc2ebac",
   "metadata": {},
   "source": [
    "### Split the data into\n",
    "The data will be split into thee groups based on event date\n",
    " * Training   - all data before 2018\n",
    " * Validation - all data during 2018\n",
    " * Test.      - all data after 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dc748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if under_sample:    \n",
    "    fraud_df = data[data[COL_FRAUD]==1]\n",
    "    non_fraud_df = data[data[COL_FRAUD]==0]\n",
    "    nr_non_fraud_samples = min((len(data) - len(fraud_df)), int(len(fraud_df)/fraud_ratio))\n",
    "    data = cudf.concat([fraud_df, non_fraud_df.sample(nr_non_fraud_samples)])\n",
    "\n",
    "training_idx = data[COL_YEAR] < 2018\n",
    "validation_idx = data[COL_YEAR] == 2018\n",
    "test_idx = data[COL_YEAR] > 2018\n",
    "\n",
    "data[COL_FRAUD].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd036929",
   "metadata": {},
   "source": [
    "### Scale numerical columns and encode categorical columns of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2d570cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As some of the encoder we want to use is not available in cuml, we can use pandas for now.\n",
    "# Move training data to pandas for preprocessing\n",
    "pdf_training = data[training_idx].to_pandas()[predictor_columns + target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d135cc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use one-hot encoding for columns with <= 8 categories, and binary encoding for columns with more categories \n",
    "columns_for_binary_encoding = []\n",
    "columns_for_onehot_encoding = []\n",
    "for col in nominal_predictors:\n",
    "    print(col, len(data[col].unique()))\n",
    "    if len(data[col].unique()) <= 8:\n",
    "        columns_for_onehot_encoding.append(col)\n",
    "    else:\n",
    "        columns_for_binary_encoding.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ee6b77fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark categorical column as \"category\"\n",
    "pdf_training[nominal_predictors] = pdf_training[nominal_predictors].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "49eb57f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoders to encode categorical columns and scalers to scale numerical columns\n",
    "\n",
    "bin_encoder = Pipeline(\n",
    "    steps=[\n",
    "        (\"binary\", BinaryEncoder(handle_missing='value', handle_unknown='value'))\n",
    "    ]\n",
    ")\n",
    "onehot_encoder = Pipeline(\n",
    "    steps=[\n",
    "        (\"onehot\", OneHotEncoder())\n",
    "    ]\n",
    ")\n",
    "std_scaler = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"standard\", StandardScaler())],\n",
    ")\n",
    "robust_scaler = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"robust\", RobustScaler())],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "613db861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compose encoders and scalers in a column transformer\n",
    "transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"binary\", bin_encoder, columns_for_binary_encoding),\n",
    "        (\"onehot\", onehot_encoder, columns_for_onehot_encoding),\n",
    "        (\"robust\", robust_scaler, [COL_AMOUNT]),\n",
    "    ], remainder=\"passthrough\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "de594998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit column transformer with training data\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "transformer = transformer.fit(pdf_training[predictor_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e3f88ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed column names\n",
    "columns_of_transformed_data = list(\n",
    "    map(lambda name: name.split('__')[1],\n",
    "        list(transformer.get_feature_names_out(predictor_columns))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2bdc0acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data type of transformed columns \n",
    "type_mapping = {}\n",
    "for col in columns_of_transformed_data:\n",
    "    if col.split('_')[0] in nominal_predictors:\n",
    "        type_mapping[col] = 'int8'\n",
    "    elif col in numerical_predictors:\n",
    "        type_mapping[col] = 'float'\n",
    "    elif col in target_column:\n",
    "        type_mapping[col] = data.dtypes.to_dict()[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "76332e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform training data\n",
    "preprocessed_training_data = transformer.transform(pdf_training[predictor_columns])\n",
    "\n",
    "# Convert transformed data to panda DataFrame\n",
    "preprocessed_training_data = pd.DataFrame(\n",
    "    preprocessed_training_data, columns=columns_of_transformed_data)\n",
    "# Copy target column\n",
    "preprocessed_training_data[COL_FRAUD] = pdf_training[COL_FRAUD].values\n",
    "preprocessed_training_data = preprocessed_training_data.astype(type_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "078b4f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the transformer \n",
    "\n",
    "with open(os.path.join(tabformer_base_path, 'preprocessor.pkl'),'wb') as f:\n",
    "    pickle.dump(transformer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e46229",
   "metadata": {},
   "source": [
    "#### Save transformed training data for XGBoost training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e3362673",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(tabformer_base_path, 'preprocessor.pkl'),'rb') as f:\n",
    "    loaded_transformer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "47f6663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform test data using the transformer fitted on training data\n",
    "pdf_test = data[test_idx].to_pandas()[predictor_columns + target_column]\n",
    "pdf_test[nominal_predictors] = pdf_test[nominal_predictors].astype(\"category\")\n",
    "\n",
    "preprocessed_test_data = loaded_transformer.transform(pdf_test[predictor_columns])\n",
    "preprocessed_test_data = pd.DataFrame(preprocessed_test_data, columns=columns_of_transformed_data)\n",
    "\n",
    "# Copy target column\n",
    "preprocessed_test_data[COL_FRAUD] = pdf_test[COL_FRAUD].values\n",
    "preprocessed_test_data = preprocessed_test_data.astype(type_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0ce80a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform validation data using the transformer fitted on training data\n",
    "pdf_validation = data[validation_idx].to_pandas()[predictor_columns + target_column]\n",
    "pdf_validation[nominal_predictors] = pdf_validation[nominal_predictors].astype(\"category\")\n",
    "\n",
    "preprocessed_validation_data = loaded_transformer.transform(pdf_validation[predictor_columns])\n",
    "preprocessed_validation_data = pd.DataFrame(preprocessed_validation_data, columns=columns_of_transformed_data)\n",
    "\n",
    "# Copy target column\n",
    "preprocessed_validation_data[COL_FRAUD] = pdf_validation[COL_FRAUD].values\n",
    "preprocessed_validation_data = preprocessed_validation_data.astype(type_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2ca66b-d3dc-4f67-9754-b90bbea6e286",
   "metadata": {},
   "source": [
    "## Write out the data for XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "89c16cfb-0bd4-4efb-a610-f3ae7445d96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training data\n",
    "out_path = os.path.join(tabformer_xgb, 'training.csv')\n",
    "if not os.path.exists(os.path.dirname(out_path)):\n",
    "  os.makedirs(os.path.dirname(out_path))\n",
    "preprocessed_training_data.to_csv(out_path, header=True, index=False, columns=columns_of_transformed_data + target_column)\n",
    "# preprocessed_training_data.to_parquet(out_path, index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f3ef6b19-062b-42d5-8caa-6f6011648b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## validation data\n",
    "out_path = os.path.join(tabformer_xgb, 'validation.csv')\n",
    "if not os.path.exists(os.path.dirname(out_path)):\n",
    "  os.makedirs(os.path.dirname(out_path))\n",
    "preprocessed_validation_data.to_csv(out_path, header=True, index=False, columns=columns_of_transformed_data + target_column)\n",
    "# preprocessed_validation_data.to_parquet(out_path, index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cdc8e3b9-841d-49f3-b3ae-3017a04605e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test data\n",
    "out_path = os.path.join(tabformer_xgb, 'test.csv')\n",
    "preprocessed_test_data.to_csv(out_path, header=True, index=False, columns=columns_of_transformed_data + target_column)\n",
    "# preprocessed_test_data.to_parquet(out_path, index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "67fb32b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write untransformed test data that has only (renamed) predictor columns and target\n",
    "out_path = os.path.join(tabformer_xgb, 'untransformed_test.csv')\n",
    "pdf_test.to_csv(out_path, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2d6cb604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete dataFrames that are not needed anymore\n",
    "del(pdf_training)\n",
    "del(pdf_validation)\n",
    "del(pdf_test)\n",
    "del(preprocessed_training_data)\n",
    "del(preprocessed_validation_data)\n",
    "del(preprocessed_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfbfd83",
   "metadata": {},
   "source": [
    "### GNN Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e518c8",
   "metadata": {},
   "source": [
    "#### Setting Vertex IDs\n",
    "In order to create a graph, the different vertices need to be assigned unique vertex IDs. Additionally, the IDs needs to be consecutive and positive.\n",
    "\n",
    "There are three nodes groups here: Transactions, Users, and Merchants. \n",
    "\n",
    "This IDs are not used in training, just used for graph processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "194a47d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same training data as used for XGBoost\n",
    "data = data[training_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0ba0cb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a lot of process has occurred, sort the data and reset the index\n",
    "data = data.sort_values(by=[COL_YEAR, COL_MONTH, COL_DAY, COL_TIME], ascending=False)\n",
    "data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2a75c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each transaction gets a unique ID\n",
    "COL_TRANSACTION_ID = 'Tx_ID'\n",
    "COL_MERCHANT_ID = 'Merchant_ID'\n",
    "COL_USER_ID = 'User_ID'\n",
    "\n",
    "# The number of transaction is the same as the size of the list, and hence the index value\n",
    "data[COL_TRANSACTION_ID] = data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "472ea57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the max transaction ID to compute first merchant ID\n",
    "max_tx_id = data[COL_TRANSACTION_ID].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8ef04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Merchant string to consecutive integers\n",
    "merchant_name_to_id = dict((v, k) for k, v in data[COL_MERCHANT].unique().to_dict().items())\n",
    "data[COL_MERCHANT_ID] = data[COL_MERCHANT].map(merchant_name_to_id) + (max_tx_id + 1)\n",
    "data[COL_MERCHANT_ID].min(), data[COL_MERCHANT].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6937df18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, get the max merchant ID to compute first user ID\n",
    "max_merchant_id = data[COL_MERCHANT_ID].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b153352c",
   "metadata": {},
   "source": [
    "##### NOTE: the 'User' and 'Card' columns of the original data were used to crate updated 'Card' colum\n",
    "* You can use user or card as nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030a2335",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert Card to consecutive IDs\n",
    "id_to_consecutive_id = dict((v, k) for k, v in data[COL_CARD].unique().to_dict().items())\n",
    "data[COL_USER_ID] = data[COL_CARD].map(id_to_consecutive_id) + max_merchant_id + 1\n",
    "data[COL_USER_ID].min(), data[COL_USER_ID].max()\n",
    "\n",
    "# id_to_consecutive_id = dict((v, k) for k, v in data[COL_USER].unique().to_dict().items())\n",
    "# data[COL_USER_ID] = data[COL_USER].map(id_to_consecutive_id) + max_merchant_id + 1\n",
    "# data[COL_USER_ID].min(), data[COL_USER].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "28858422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the max user ID\n",
    "max_user_id = data[COL_USER_ID].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903e5115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the the transaction, merchant and user ids are consecutive\n",
    "id_range = data[COL_TRANSACTION_ID].min(), data[COL_TRANSACTION_ID].max()\n",
    "print(f'Transaction ID range {id_range}')\n",
    "id_range = data[COL_MERCHANT_ID].min(), data[COL_MERCHANT_ID].max()\n",
    "print(f'Merchant ID range {id_range}')\n",
    "id_range = data[COL_USER_ID].min(), data[COL_USER_ID].max()\n",
    "print(f'User ID range {id_range}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f2d0dfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "assert( data[COL_TRANSACTION_ID].max() == data[COL_MERCHANT_ID].min() - 1)\n",
    "assert( data[COL_MERCHANT_ID].max() == data[COL_USER_ID].min() - 1)\n",
    "assert(len(data[COL_USER_ID].unique()) == (data[COL_USER_ID].max() - data[COL_USER_ID].min() + 1))\n",
    "assert(len(data[COL_MERCHANT_ID].unique()) == (data[COL_MERCHANT_ID].max() - data[COL_MERCHANT_ID].min() + 1))\n",
    "assert(len(data[COL_TRANSACTION_ID].unique()) == (data[COL_TRANSACTION_ID].max() - data[COL_TRANSACTION_ID].min() + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9c3df3-a5be-4899-8bf9-6152aca114c7",
   "metadata": {},
   "source": [
    "### Write out the data for GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b86862-d129-4ece-a60d-dc798f3a68b5",
   "metadata": {},
   "source": [
    "#### Create the Graph Edge Data file \n",
    "The file is in COO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b288c5a7-20dd-40ff-b0eb-7a5895bcc464",
   "metadata": {},
   "outputs": [],
   "source": [
    "COL_GRAPH_SRC = 'src'\n",
    "COL_GRAPH_DST = 'dst'\n",
    "COL_GRAPH_WEIGHT = 'wgt'\n",
    "\n",
    "# User to Transactions\n",
    "U_2_T = cudf.DataFrame()\n",
    "U_2_T[COL_GRAPH_SRC] = data[COL_USER_ID]\n",
    "U_2_T[COL_GRAPH_DST] = data[COL_TRANSACTION_ID]\n",
    "if make_undirected:\n",
    "  T_2_U = cudf.DataFrame()\n",
    "  T_2_U[COL_GRAPH_SRC] = data[COL_TRANSACTION_ID]\n",
    "  T_2_U[COL_GRAPH_DST] = data[COL_USER_ID]\n",
    "  U_2_T = cudf.concat([U_2_T, T_2_U])\n",
    "  del T_2_U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a970747d-07a2-43b3-b39c-19a0196fa5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transactions to Merchants\n",
    "T_2_M = cudf.DataFrame()\n",
    "T_2_M[COL_GRAPH_SRC] = data[COL_MERCHANT_ID]\n",
    "T_2_M[COL_GRAPH_DST] = data[COL_TRANSACTION_ID]\n",
    "\n",
    "if make_undirected:\n",
    "  M_2_T = cudf.DataFrame()\n",
    "  M_2_T[COL_GRAPH_SRC] = data[COL_TRANSACTION_ID]\n",
    "  M_2_T[COL_GRAPH_DST] = data[COL_MERCHANT_ID]\n",
    "  T_2_M = cudf.concat([T_2_M, M_2_T])\n",
    "  del M_2_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e704fd-ae9f-45b1-ad56-bdc0b743d09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Edge = cudf.concat([U_2_T, T_2_M])\n",
    "Edge[COL_GRAPH_WEIGHT] = 0.0\n",
    "len(Edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c74572f6-ff6e-4c8f-803e-0ae2c0587c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now write out the data\n",
    "out_path = os.path.join (tabformer_gnn, 'edges.csv')\n",
    "\n",
    "if not os.path.exists(os.path.dirname(out_path)):\n",
    "  os.makedirs(os.path.dirname(out_path))\n",
    "  \n",
    "Edge.to_csv(out_path, header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3dd3ff45-3796-4069-9e3a-587743c4e1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(Edge)\n",
    "del(U_2_T)\n",
    "del(T_2_M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed00c481-1737-4152-9d23-f3cb24f2adcd",
   "metadata": {},
   "source": [
    "### Now the feature data\n",
    "Feature data needs to be is sorted in order, where the row index corresponds to the node ID\n",
    "\n",
    "The data is comprised of three sets of features\n",
    "* Transactions\n",
    "* Users\n",
    "* Merchants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805c9d23",
   "metadata": {},
   "source": [
    "#### To get feature vectors of Transaction nodes, transform the training data using pre-fitted transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "584fe9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_feature_df = pd.DataFrame(\n",
    "    loaded_transformer.transform(\n",
    "        data[predictor_columns].to_pandas()\n",
    "        ),\n",
    "    columns=columns_of_transformed_data).astype(type_mapping)\n",
    "\n",
    "node_feature_df[COL_FRAUD] = data[COL_FRAUD].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aa8f86",
   "metadata": {},
   "source": [
    "#### For graph nodes associated with merchant and user, add feature vectors of zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b35f9f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of graph nodes for users and merchants \n",
    "nr_users_and_merchant_nodes = max_user_id - max_tx_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b5d312bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not spread_features:\n",
    "    # Create feature vector of all zeros for each user and merchant node\n",
    "    empty_feature_df = cudf.DataFrame(\n",
    "        columns=columns_of_transformed_data + target_column,\n",
    "        dtype='int8', \n",
    "        index=range(nr_users_and_merchant_nodes)\n",
    "    )\n",
    "    empty_feature_df = empty_feature_df.fillna(0)\n",
    "    empty_feature_df=empty_feature_df.astype(type_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a72d3ea5-e04f-4af1-a0e0-09964555c1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not spread_features:\n",
    "    # Concatenate transaction features followed by features for merchants and user nodes\n",
    "    node_feature_df = pd.concat([node_feature_df, empty_feature_df.to_pandas()]).astype(type_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a364d173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User specific columns\n",
    "if spread_features:\n",
    "    user_specific_columns = [COL_CARD, COL_CHIP]\n",
    "    user_specific_columns_of_transformed_data = []\n",
    "\n",
    "    for col in node_feature_df.columns:\n",
    "        if col.split('_')[0] in user_specific_columns:\n",
    "            user_specific_columns_of_transformed_data.append(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "92d88c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merchant specific columns\n",
    "if spread_features:\n",
    "    merchant_specific_columns = [COL_MERCHANT, COL_CITY, COL_ZIP, COL_MCC]\n",
    "    merchant_specific_columns_of_transformed_data = []\n",
    "    \n",
    "    for col in node_feature_df.columns:\n",
    "        if col.split('_')[0] in merchant_specific_columns:\n",
    "            merchant_specific_columns_of_transformed_data.append(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f62755ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction specific columns\n",
    "if spread_features:\n",
    "    transaction_specific_columns = list(\n",
    "        set(numerical_predictors).union(nominal_predictors)\n",
    "        - set(user_specific_columns).union(merchant_specific_columns))\n",
    "    transaction_specific_columns_of_transformed_data = []\n",
    "    \n",
    "    for col in node_feature_df.columns:\n",
    "        if col.split('_')[0] in transaction_specific_columns:\n",
    "            transaction_specific_columns_of_transformed_data.append(col)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12061da",
   "metadata": {},
   "source": [
    "#### Construct feature vector for merchants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "de484a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "if spread_features:\n",
    "    # Find indices of unique merchants\n",
    "    idx_df = cudf.DataFrame()\n",
    "    idx_df[COL_MERCHANT_ID] =  data[COL_MERCHANT_ID]\n",
    "    idx_df = idx_df.sort_values(by=COL_MERCHANT_ID)\n",
    "    idx_df = idx_df.drop_duplicates(subset=COL_MERCHANT_ID)\n",
    "    assert((data.iloc[idx_df.index][COL_MERCHANT_ID] == idx_df[COL_MERCHANT_ID]).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5be790eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if spread_features:\n",
    "    # Copy merchant specific columns, and set the rest to zero\n",
    "    merchant_specific_feature_df = node_feature_df.iloc[idx_df.index.to_numpy()]\n",
    "    merchant_specific_feature_df.\\\n",
    "    loc[:, \n",
    "        transaction_specific_columns_of_transformed_data +\n",
    "          user_specific_columns_of_transformed_data] = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "576091c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if spread_features:\n",
    "    # Find indices of unique users\n",
    "    idx_df = cudf.DataFrame()\n",
    "    idx_df[COL_USER_ID] = data[COL_USER_ID]\n",
    "    idx_df = idx_df.sort_values(by=COL_USER_ID)\n",
    "    idx_df = idx_df.drop_duplicates(subset=COL_USER_ID)\n",
    "    assert((data.iloc[idx_df.index][COL_USER_ID] == idx_df[COL_USER_ID]).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "aec23ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if spread_features:\n",
    "    # Copy user specific columns, and set the rest to zero\n",
    "    user_specific_feature_df = node_feature_df.iloc[idx_df.index.to_numpy()]\n",
    "    user_specific_feature_df.\\\n",
    "    loc[:,\n",
    "        transaction_specific_columns_of_transformed_data +\n",
    "          merchant_specific_columns_of_transformed_data] = 0.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8296a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate features of node, user and merchant\n",
    "if spread_features:\n",
    "    \n",
    "    node_feature_df[merchant_specific_columns_of_transformed_data] = 0.0\n",
    "    node_feature_df[user_specific_columns_of_transformed_data] = 0.0\n",
    "    node_feature_df = pd.concat(\n",
    "        [node_feature_df, merchant_specific_feature_df, user_specific_feature_df]\n",
    "        ).astype(type_mapping)\n",
    "    \n",
    "    # features to save\n",
    "    node_feature_df = node_feature_df[\n",
    "        transaction_specific_columns_of_transformed_data +\n",
    "        merchant_specific_columns_of_transformed_data +\n",
    "        user_specific_columns_of_transformed_data + [COL_FRAUD]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "527f6ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# target labels to save\n",
    "label_df = node_feature_df[[COL_FRAUD]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "15e1cba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove target label from feature vectors\n",
    "_ = node_feature_df.drop(columns=[COL_FRAUD], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310d9500",
   "metadata": {},
   "source": [
    "#### Write out node features and target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "139bfd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write node target label to csv file\n",
    "out_path = os.path.join(tabformer_gnn, 'labels.csv')\n",
    "\n",
    "if not os.path.exists(os.path.dirname(out_path)):\n",
    "  os.makedirs(os.path.dirname(out_path))\n",
    "\n",
    "label_df.to_csv(out_path, header=False, index=False)\n",
    "# label_df.to_parquet(out_path, index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b8fe801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write node features to csv file\n",
    "out_path = os.path.join(tabformer_gnn, 'features.csv')\n",
    "\n",
    "if not os.path.exists(os.path.dirname(out_path)):\n",
    "  os.makedirs(os.path.dirname(out_path))\n",
    "node_feature_df[columns_of_transformed_data].to_csv(out_path, header=True, index=False)\n",
    "# node_feature_df.to_parquet(out_path, index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fbe75d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete dataFrames\n",
    "del data\n",
    "del node_feature_df\n",
    "del label_df\n",
    "\n",
    "if spread_features:\n",
    "    del merchant_specific_feature_df\n",
    "    del user_specific_feature_df\n",
    "else:\n",
    "    del empty_feature_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6afd9b",
   "metadata": {},
   "source": [
    "#### Number of transaction nodes in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f5bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of transaction nodes, needed for GNN training\n",
    "nr_transaction_nodes = max_tx_id + 1\n",
    "nr_transaction_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275bfc8b",
   "metadata": {},
   "source": [
    "#### Maximum number of cards per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867661d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max number of cards per user, needed for inference\n",
    "max_nr_cards_per_user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5434a7",
   "metadata": {},
   "source": [
    "#### Save variable for training and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9d741c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "variables_to_save = {\n",
    "    k: v for k, v in globals().items() if isinstance(v, (str, int)) and k.startswith('COL_')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "86727cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_to_save['NUM_TRANSACTION_NODES'] = int(nr_transaction_nodes)\n",
    "variables_to_save['MAX_NR_CARDS_PER_USER'] = int(max_nr_cards_per_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6a59a5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary to a JSON file\n",
    "\n",
    "with open(os.path.join(tabformer_base_path, 'variables.json'), 'w') as json_file:\n",
    "    json.dump(variables_to_save, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2f6f28",
   "metadata": {},
   "source": [
    "## That's it!\n",
    "The data is now ready for processing\n",
    "\n",
    "## Copyright and License\n",
    "<hr/>\n",
    "Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.\n",
    "\n",
    "<br/>\n",
    "\n",
    " Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    " you may not use this file except in compliance with the License.\n",
    " You may obtain a copy of the License at\n",
    " \n",
    " http://www.apache.org/licenses/LICENSE-2.0\n",
    " \n",
    " Unless required by applicable law or agreed to in writing, software\n",
    " distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    " WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    " See the License for the specific language governing permissions and\n",
    " limitations under the License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
