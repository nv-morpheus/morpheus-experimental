{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a GNN-based XGBoost Model\n",
    "#### Goals\n",
    "* Train a GNN (GraphSAGE) model that produces node (transaction) embeddings.\n",
    "* Use these node embeddings to train an XGBoost model.\n",
    "* Save the trained GNN and XGBoost models for inference.\n",
    "\n",
    "__Prerequisite__: The preprocessing notebook must be executed before running this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the datasets to choose from\n",
    "TABFORMER = \"TabFormer\"\n",
    "SPARKOV = \"Sparkov\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the dataset to train the models on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__:  This notebook works for both __TabFormer__ and __Sparkov__ dataset. \n",
    "Make sure that the right dataset is selected.\n",
    "For yhe TabFormer dataset, set\n",
    "\n",
    "```code\n",
    "    DATASET = TABFORMER\n",
    "```\n",
    "and for the Sparkov dataset, set\n",
    "\n",
    "```code\n",
    "    DATASET = SPARKOV\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to either TABFORMER or SPARKOV\n",
    "DATASET = TABFORMER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Import necessary libraries, packages, and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General-purpose libraries and OS handling\n",
    "import os\n",
    "from typing import Tuple, Dict\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# GPU-accelerated libraries (torch, cupy, cudf, rmm)\n",
    "import torch\n",
    "import cupy\n",
    "import cudf\n",
    "import rmm\n",
    "from rmm.allocators.cupy import rmm_cupy_allocator\n",
    "from rmm.allocators.torch import rmm_torch_allocator\n",
    "\n",
    "# Reinitialize RMM and set allocators to manage memory efficiently on GPU\n",
    "rmm.reinitialize(devices=[0], pool_allocator=True, managed_memory=True)\n",
    "cupy.cuda.set_allocator(rmm_cupy_allocator)\n",
    "torch.cuda.memory.change_current_allocator(rmm_torch_allocator)\n",
    "\n",
    "# PyTorch and related libraries\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "# PyTorch Geometric and cuGraph libraries for GNNs and graph handling\n",
    "import cugraph_pyg\n",
    "from cugraph_pyg.loader import NeighborLoader\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "# Enable GPU memory spilling to CPU with cuDF to handle larger datasets\n",
    "from cugraph.testing.mg_utils import enable_spilling  # noqa: E402\n",
    "enable_spilling()\n",
    "\n",
    "# XGBoost for machine learning model building\n",
    "import xgboost as xgb\n",
    "\n",
    "# Numerical operations with cupy and numpy\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning metrics from sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some config parameters for neighborhood sampler and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = type('', (), {})()\n",
    "\n",
    "args.out_channels = 2\n",
    "args.batch_size = 1024\n",
    "args.fan_out = 10\n",
    "args.use_cross_weights = True\n",
    "args.cross_weights = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Path to pre-processed data and directory to save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateset_name_to_path= defaultdict(lambda: \"../data/TabFormer\")\n",
    "\n",
    "dateset_name_to_path['TabFormer'] = '../data/TabFormer'\n",
    "dateset_name_to_path['Sparkov'] = '../data/Sparkov'\n",
    "args.dataset_base_path = dateset_name_to_path[DATASET]\n",
    "\n",
    "args.dataset_root = os.path.join(args.dataset_base_path, 'gnn')\n",
    "args.model_root_dir = os.path.join(args.dataset_base_path, 'models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read number of transactions nodes that was saved during preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Number of transactions nodes were saved in variables.json during training\n",
    "with open(os.path.join(args.dataset_base_path, 'variables.json'), 'r') as json_file:\n",
    "    num_transaction_nodes = json.load(json_file)['NUM_TRANSACTION_NODES']\n",
    "\n",
    "num_transaction_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a GraphSAGE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    GraphSAGE model for graph-based learning.\n",
    "\n",
    "    This model learns node embeddings by aggregating information from a node's \n",
    "    neighborhood using multiple graph convolutional layers.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    in_channels : int\n",
    "        The number of input features for each node.\n",
    "    hidden_channels : int\n",
    "        The number of hidden units in each layer, controlling the embedding dimension.\n",
    "    out_channels : int\n",
    "        The number of output features (or classes) for the final layer.\n",
    "    n_hops : int\n",
    "        The number of GraphSAGE layers (or hops) used to aggregate information \n",
    "        from neighboring nodes.\n",
    "    dropout_prob : float, optional (default=0.25)\n",
    "        The probability of dropping out nodes during training for regularization.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, n_hops, dropout_prob=0.25):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "\n",
    "        # list of conv layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        # add first conv layer to the list\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        # add the remaining conv layers to the list\n",
    "        for _ in range(n_hops - 1):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        \n",
    "        # output layer\n",
    "        self.fc = nn.Linear(hidden_channels, out_channels)        \n",
    "\n",
    "    def forward(self, x, edge_index, return_hidden=False):\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "            \n",
    "        if return_hidden:\n",
    "            return x\n",
    "        else:\n",
    "            return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Define a function to train the GraphSAGE model\n",
    "__Note__: This function is called a few times if grid search is used to find better hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gnn(model, loader, optimizer, criterion)->float:\n",
    "    \"\"\"\n",
    "    Trains the GraphSAGE model for one epoch.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        The GNN model to be trained.\n",
    "    loader : tcugraph_pyg.loader.NeighborLoader\n",
    "        DataLoader that provides batches of graph data for training.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        Optimizer used to update the model's parameters.\n",
    "    criterion : torch.nn.Module\n",
    "        Loss function used to calculate the difference between predictions and targets.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    float\n",
    "        The average training loss over all batches for this epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    batch_count = 0\n",
    "    for batch in loader:\n",
    "        batch_count += 1\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_size = batch.batch_size\n",
    "        out = model(batch.x[:,:].to(torch.float32), batch.edge_index)[:batch_size]\n",
    "        y = batch.y[:batch_size].view(-1).to(torch.long)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / batch_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Define a function to extract node (transaction) embeddings from the second-to-last layer of the GraphSAGE model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_embeddings(model, loader)->Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Extracts node embeddings produced by the GraphSAGE model.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        The model used to generate embeddings, typically a pre-trained neural network.\n",
    "    loader : cugraph_pyg.loader.NeighborLoader\n",
    "        NeighborLoader that provides batches of data for embedding extraction.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    Tuple[torch.Tensor, torch.Tensor]\n",
    "        A tuple containing two tensors:\n",
    "        - embeddings: A tensor containing embeddings for each input sample in the dataset.\n",
    "        - labels: A tensor containing the corresponding labels for each sample.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch_size = batch.batch_size\n",
    "            hidden = model(batch.x[:,:].to(torch.float32), batch.edge_index, return_hidden=True)[:batch_size]\n",
    "            embeddings.append(hidden)  # Keep embeddings on GPU\n",
    "            labels.append(batch.y[:batch_size].view(-1).to(torch.long))\n",
    "    embeddings = torch.cat(embeddings, dim=0)  # Concatenate embeddings on GPU\n",
    "    labels = torch.cat(labels, dim=0)  # Concatenate labels on GPU\n",
    "    return embeddings, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Define a function to evaluate the GraphSAGE model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_gnn(model, loader) -> float:\n",
    "    \"\"\"\n",
    "    Evaluates the performance of the GraphSAGE model.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        The GNN model to be evaluated.\n",
    "    loader : cugraph_pyg.loader.NeighborLoader\n",
    "        NeighborLoader that provides batches of data for evaluation.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    float\n",
    "        The average f1-score computed over all batches.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_pos_seen = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "\n",
    "            batch_size = batch.batch_size\n",
    "            out = model(batch.x[:,:].to(torch.float32), batch.edge_index)[:batch_size]\n",
    "            preds = out.argmax(dim=1)\n",
    "            y = batch.y[:batch_size].view(-1).to(torch.long)\n",
    "            \n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(y.cpu().numpy())\n",
    "            total_pos_seen += (y.cpu().numpy()==1).sum()\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "    # roc_auc = roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "    print(f\"\\nGNN Model Evaluation:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    # print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function to compute validation loss GraphSAGE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validation_loss(model, loader, criterion)->float:\n",
    "    \"\"\"\n",
    "    Computes the average validation loss for the GraphSAGE model.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        The model for which the validation loss is calculated.\n",
    "    loader : cugraph_pyg.loader.NeighborLoader\n",
    "        NeighborLoader that provides batches of validation data.\n",
    "    criterion : torch.nn.Module\n",
    "        Loss function used to compute the loss between predictions and targets.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    float\n",
    "        The average validation loss over all batches.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        for batch in loader:\n",
    "            batch_count += 1\n",
    "            batch_size = batch.batch_size\n",
    "            out = model(batch.x[:,:].to(torch.float32), batch.edge_index)[:batch_size]\n",
    "            y = batch.y[:batch_size].view(-1).to(torch.long)\n",
    "            loss = criterion(out, y)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / batch_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Define a function to train a XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.dlpack import to_dlpack\n",
    "\n",
    "def train_xgboost(embeddings, labels)->xgb.Booster:\n",
    "    \"\"\"\n",
    "    Trains an XGBoost classifier on the provided embeddings and labels.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    embeddings : torch.Tensor\n",
    "        The input feature embeddings for transaction nodes.\n",
    "    labels : torch.Tensor\n",
    "        The target labels (Fraud or Non-fraud) transaction, with the same length as the number of \n",
    "        rows in `embeddings`.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    xgboost.Booster\n",
    "        A trained XGBoost model fitted on the provided data.\n",
    "    \"\"\"\n",
    "\n",
    "    labels_cudf = cudf.Series(cp.from_dlpack(to_dlpack(labels)))\n",
    "    embeddings_cudf = cudf.DataFrame(cp.from_dlpack(to_dlpack(embeddings)))\n",
    "\n",
    "    # Convert data to DMatrix format for XGBoost on GPU\n",
    "    dtrain = xgb.DMatrix(embeddings_cudf, label=labels_cudf)\n",
    "\n",
    "    # Set XGBoost parameters for GPU usage\n",
    "    param = {\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.2,\n",
    "        'objective': 'binary:logistic',  # Binary classification\n",
    "        'eval_metric': 'logloss',\n",
    "        'tree_method': 'hist',  # Use GPU\n",
    "        'device': 'cuda'\n",
    "    }\n",
    "\n",
    "    # Train the XGBoost model\n",
    "    bst = xgb.train(param, dtrain, num_boost_round=100)\n",
    "    \n",
    "    return bst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "\n",
    "#### Define a function to evaluate the XGBoost model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from cuml.metrics import confusion_matrix\n",
    "\n",
    "def evaluate_xgboost(bst, embeddings, labels):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of a XGBoost model by calculating different metrics.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    bst : xgboost.Booster\n",
    "        The trained XGBoost model to be evaluated.\n",
    "    embeddings : torch.Tensor\n",
    "        The input feature embeddings for transaction nodes.\n",
    "    labels : torch.Tensor\n",
    "        The target labels (Fraud or Non-fraud) transaction, with the same length as the number of \n",
    "        rows in `embeddings`.\n",
    "    Returns:\n",
    "    -------\n",
    "    A tuple containing f1-score, recall, precision, accuracy and the confusion matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert embeddings to cuDF DataFrame\n",
    "    embeddings_cudf = cudf.DataFrame(cp.from_dlpack(to_dlpack(embeddings)))\n",
    "    \n",
    "    # Create DMatrix for the test embeddings\n",
    "    dtest = xgb.DMatrix(embeddings_cudf)\n",
    "    \n",
    "    # Predict using XGBoost on GPU\n",
    "    preds = bst.predict(dtest)\n",
    "    pred_labels = (preds > 0.5).astype(int)\n",
    "\n",
    "    # Move labels to CPU for evaluation\n",
    "    labels_cpu = labels.cpu().numpy()\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    accuracy = accuracy_score(labels_cpu, pred_labels)\n",
    "    precision = precision_score(labels_cpu, pred_labels, zero_division=0)\n",
    "    recall = recall_score(labels_cpu, pred_labels, zero_division=0)\n",
    "    f1 = f1_score(labels_cpu, pred_labels, zero_division=0)\n",
    "    roc_auc = roc_auc_score(labels_cpu, preds)\n",
    "    conf_mat = confusion_matrix(labels.cpu().numpy(), pred_labels)\n",
    "    \n",
    "    return f1, recall, precision, accuracy, conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a class to stop training once the model stops improving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    EarlyStopping class to halt training when a monitored metric stops improving.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    patience : int, optional (default=10)\n",
    "        The number of epochs with no improvement after which training will be stopped.\n",
    "    min_delta : float, optional (default=0)\n",
    "        The minimum change in the monitored metric to qualify as an improvement. \n",
    "        If the change is smaller than `min_delta`, it is considered as no improvement.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=10, min_delta=0):\n",
    "        \n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "\n",
    "    def check_early_stopping(self, val_loss):\n",
    "\n",
    "        if self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0  # Reset counter if there's an improvement\n",
    "        else:\n",
    "            self.counter += 1  # Increment counter if no improvement\n",
    "            \n",
    "        if self.counter >= self.patience:\n",
    "            return True\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to load data and create graph\n",
    "* loads edges and create graph using cugraph-pyg\n",
    "* loads preprocessed features associated with the graph nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(\n",
    "    dataset_root : str,\n",
    "    edge_filename: str = 'edges.csv',\n",
    "    label_filename: str = 'labels.csv',\n",
    "    node_feature_filename: str = 'features.csv',\n",
    "    has_edge_feature: bool = False,\n",
    "    edge_src_col: str = 'src',\n",
    "    edge_dst_col: str = 'dst',\n",
    "    edge_att_col: str = 'type'\n",
    ") -> Tuple[\n",
    "    Tuple[torch_geometric.data.FeatureStore, torch_geometric.data.GraphStore],\n",
    "    Dict[str, torch.Tensor],\n",
    "    int,\n",
    "    int,\n",
    "]:\n",
    "    # Load the Graph data\n",
    "    edge_path   = os.path.join(dataset_root, edge_filename)\n",
    "    edge_data = cudf.read_csv(edge_path, header=None, names=[edge_src_col, edge_dst_col, edge_att_col], dtype=['int32','int32','float'])\n",
    "    \n",
    "    num_nodes = max(edge_data[edge_src_col].max(), edge_data[ edge_dst_col].max()) + 1 \n",
    "    src_tensor = torch.as_tensor(edge_data[edge_src_col], device='cuda')\n",
    "    dst_tensor = torch.as_tensor(edge_data[edge_dst_col], device='cuda')\n",
    "\n",
    "    \n",
    "\n",
    "    graph_store = cugraph_pyg.data.GraphStore()\n",
    "    graph_store[(\"n\", \"e\", \"n\"), \"coo\", False, (num_nodes, num_nodes)] = [src_tensor, dst_tensor] \n",
    "\n",
    "    \n",
    "    edge_feature_store = None\n",
    "    if has_edge_feature:\n",
    "        from cugraph_pyg.data import  TensorDictFeatureStore\n",
    "        edge_feature_store = TensorDictFeatureStore()\n",
    "        edge_attr = torch.as_tensor(edge_data[edge_att_col], device='cuda')\n",
    "        edge_feature_store[(\"n\", \"e\", \"n\"), \"rel\"] = edge_attr.unsqueeze(1)\n",
    " \n",
    "    \n",
    "    del(edge_data)\n",
    "    \n",
    "    # load the label\n",
    "    label_path   = os.path.join (dataset_root, label_filename)\n",
    "    label_data = cudf.read_csv(label_path, header=None, dtype=['int32'])\n",
    "    y_label_tensor = torch.as_tensor(label_data['0'], device='cuda')\n",
    "    num_classes = label_data['0'].unique().count()\n",
    "\n",
    "    wt_data = None\n",
    "    if (args.use_cross_weights):\n",
    "        if (args.cross_weights is None):\n",
    "            num_labels_rows = label_data.size\n",
    "            counts = label_data.value_counts()\n",
    "            wt_data = torch.as_tensor(counts.sum()/counts, device='cuda', dtype=torch.float32)\n",
    "            wt_data = wt_data/wt_data.sum()\n",
    "\n",
    "            if (num_classes > 2):\n",
    "                wt_data = wt_data.T\n",
    "        else:\n",
    "            wt_data = torch.as_tensor(args.cross_weights, device='cuda')\n",
    "\n",
    "    del(label_data)\n",
    "    \n",
    "    # load the features\n",
    "    feature_path   = os.path.join(dataset_root, node_feature_filename)\n",
    "    feature_data = cudf.read_csv(feature_path)\n",
    "    \n",
    "    feature_columns = feature_data.columns\n",
    "    \n",
    "    col_tensors = []\n",
    "    for c in feature_columns:\n",
    "        t = torch.as_tensor(feature_data[c].values, device='cuda')\n",
    "        col_tensors.append(t)\n",
    "\n",
    "    x_feature_tensor = torch.stack(col_tensors).T\n",
    "\n",
    "    \n",
    "    feature_store = cugraph_pyg.data.TensorDictFeatureStore()\n",
    "    feature_store[\"node\", \"x\"] = x_feature_tensor\n",
    "    feature_store[\"node\", \"y\"] = y_label_tensor\n",
    "\n",
    "    num_features = len(feature_columns)\n",
    "    \n",
    "    return (\n",
    "        (feature_store, graph_store),\n",
    "        edge_feature_store,\n",
    "        num_nodes,\n",
    "        num_features,\n",
    "        num_classes,\n",
    "        wt_data,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Define a function to train the GraphSAGE model for particular values of hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model_with_config(params, verbose=False):\n",
    "\n",
    "    data, ef_store, num_nodes, num_features, num_classes, cross_wt_data = load_data(args.dataset_root)\n",
    "    \n",
    "    num_folds = params['n_splits']  # Number of folds\n",
    "    fold_size = num_transaction_nodes // num_folds\n",
    "\n",
    "    # Perform cross-validation\n",
    "    validation_losses = []\n",
    "    for k in range(num_folds):\n",
    "        training_nodes = torch.cat(\n",
    "            (\n",
    "                torch.arange(0, k * fold_size).unsqueeze(dim=0),\n",
    "                torch.arange((k+1) * fold_size, num_transaction_nodes).unsqueeze(dim=0)\n",
    "            ),\n",
    "            dim=1\n",
    "        ).squeeze(0)\n",
    "\n",
    "        validation_nodes = torch.arange(k * fold_size, (k+1) * fold_size)\n",
    "        \n",
    "        # Create NeighborLoader for both training and testing (using cuGraph NeighborLoader)\n",
    "        train_loader = NeighborLoader(\n",
    "            data,\n",
    "            num_neighbors=[args.fan_out, args.fan_out],\n",
    "            batch_size=args.batch_size,\n",
    "            input_nodes= training_nodes,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        # Use same graph but different seed nodes\n",
    "        validation_loader = NeighborLoader(\n",
    "            data,\n",
    "            num_neighbors=[args.fan_out, args.fan_out],\n",
    "            batch_size=args.batch_size,\n",
    "            input_nodes= validation_nodes,\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Define the model\n",
    "        model = GraphSAGE(\n",
    "            in_channels=num_features,\n",
    "            hidden_channels=params['hidden_channels'],\n",
    "            out_channels=args.out_channels,\n",
    "            n_hops=params['n_hops'],\n",
    "            dropout_prob=0.25).to(device)\n",
    "\n",
    "\n",
    "        # Define optimizer and loss function for GNN\n",
    "        optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                    lr=params['learning_rate'],\n",
    "                                    weight_decay=params['weight_decay'])\n",
    "\n",
    "        # criterion = torch.nn.CrossEntropyLoss(\n",
    "        #     weight=cross_wt_data).to(device)  # Weighted loss function\n",
    "        \n",
    "        criterion = torch.nn.CrossEntropyLoss(\n",
    "            weight=torch.tensor([0.1, 0.9], dtype=torch.float32)).to(device)  # Weighted loss function\n",
    "\n",
    "        # Set up the early stopping object\n",
    "        early_stopping = EarlyStopping(patience=3, min_delta=0.01)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        num_epoch_for_best_loss = 0\n",
    "\n",
    "        # Train the GNN model\n",
    "        for epoch in range(params['num_epochs']):\n",
    "            train_loss = train_gnn(model, train_loader, optimizer, criterion)\n",
    "            val_loss = validation_loss(model, validation_loader, criterion)\n",
    "            if verbose:\n",
    "                print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "            # Check early stopping criteria\n",
    "            if early_stopping.check_early_stopping(val_loss):\n",
    "                if verbose:\n",
    "                    print(f\"Early stopping triggered at epoch {epoch+1}.\")\n",
    "                break\n",
    "\n",
    "            # Save the best model based on validation loss\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                num_epoch_for_best_loss = epoch\n",
    "        # Save validation loss for the current fold\n",
    "        validation_losses.append(best_val_loss)\n",
    "    return np.mean(validation_losses), model, num_epoch_for_best_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Parameter grid to search for better hyper-parameters\n",
    "\n",
    "__Note__: To execute the notebook faster, we commented out the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment this cell to find the best hyperparameters in the parameter grid\n",
    "# from sklearn.model_selection import ParameterGrid\n",
    "# # Define the hyperparameter grid\n",
    "# param_grid = {\n",
    "#     'n_splits': [5],\n",
    "#     'n_hops': [1, 2],\n",
    "#     'learning_rate': [0.005, 0.01],\n",
    "#     'hidden_channels': [32, 64],\n",
    "#     'num_epochs': [8, 16],\n",
    "#     'weight_decay': [1e-5],\n",
    "     \n",
    "# }\n",
    "# grid = list(ParameterGrid(param_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search for better hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment this cell to find the best hyperparameters in the parameter grid\n",
    "# best_val_loss = float('inf')\n",
    "# epoch = 0\n",
    "# best_params = None\n",
    "# for params in grid:\n",
    "#     val_loss, _, epoch = train_model_with_config(params, verbose=False)\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_params = params\n",
    "#         best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment out this cell to train on new dataset  \n",
    "best_params = {\n",
    "    'n_hops': 1,\n",
    "    'learning_rate': 0.005,\n",
    "    'hidden_channels': 32,\n",
    "    'num_epochs': 16,\n",
    "    'weight_decay': 1e-5,     \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and save the GraphSAGE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data, ef_store, num_nodes, num_features, num_classes, cross_wt_data = load_data(args.dataset_root)\n",
    "\n",
    "# Train on entire dataset\n",
    "train_loader = NeighborLoader(\n",
    "    data,\n",
    "    num_neighbors=[args.fan_out, args.fan_out],\n",
    "    batch_size=args.batch_size,\n",
    "    input_nodes= torch.arange(num_transaction_nodes),\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "# Define the model\n",
    "model = GraphSAGE(\n",
    "    in_channels=num_features,\n",
    "    hidden_channels=best_params['hidden_channels'],\n",
    "    out_channels=args.out_channels,\n",
    "    n_hops=best_params['n_hops'],\n",
    "    dropout_prob=0.25).to(device)\n",
    "\n",
    "\n",
    "# Define optimizer and loss function for GNN\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                            lr=best_params['learning_rate'],\n",
    "                            weight_decay=best_params['weight_decay'])\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(\n",
    "    weight=torch.tensor([0.1, 0.9], dtype=torch.float32)).to(device)  # Weighted loss function\n",
    "\n",
    "# Set up the early stopping object\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.01)\n",
    "\n",
    "best_train_loss = float('inf')\n",
    "\n",
    "# Train the GNN model\n",
    "\n",
    "for epoch in range(best_params['num_epochs']):\n",
    "    train_loss = train_gnn(model, train_loader, optimizer, criterion)\n",
    "    \n",
    "    # Check early stopping criteria\n",
    "    if early_stopping.check_early_stopping(train_loss):\n",
    "        print(f\"Early stopping triggered at epoch {epoch+1}.\")\n",
    "        break\n",
    "\n",
    "    # Save the best model based on validation loss\n",
    "    if train_loss < best_train_loss:\n",
    "        best_train_loss = train_loss\n",
    "        if not os.path.exists(args.model_root_dir):\n",
    "            os.makedirs(args.model_root_dir)\n",
    "        torch.save(model, os.path.join(args.model_root_dir, 'node_embedder.pth'))\n",
    "\n",
    "        print(f\"Model saved at epoch {epoch+1} with training loss {best_train_loss:.4f}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the XGBoost model based on embeddings produced by the GraphSAGE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NeighborLoader for training data\n",
    "\n",
    "data, ef_store, num_nodes, num_features, num_classes, cross_wt_data = load_data(args.dataset_root)\n",
    "\n",
    "train_loader = NeighborLoader(\n",
    "    data,\n",
    "    num_neighbors=[args.fan_out, args.fan_out],\n",
    "    batch_size=args.batch_size,\n",
    "    input_nodes= torch.arange(num_transaction_nodes),\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to GPU if available; otherwise, default to CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Extract embeddings from the second-to-last layer and keep them on GPU\n",
    "embeddings, labels = extract_embeddings(model, train_loader)\n",
    "\n",
    "# Train an XGBoost model on the extracted embeddings (on GPU)\n",
    "bst = train_xgboost(embeddings.to(device), labels.to(device))\n",
    "            \n",
    "xgb_model_path = os.path.join(args.model_root_dir, 'embedding_based_xgb_model.json')\n",
    "\n",
    "if not os.path.exists(os.path.dirname(xgb_model_path)):\n",
    "    os.makedirs(os.path.dirname(xgb_model_path))\n",
    "\n",
    "bst.save_model(xgb_model_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation the model on unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load and prepare test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "test_path = os.path.join(args.dataset_base_path, 'xgb/test.csv')\n",
    "test_data = cudf.read_csv(test_path)\n",
    "\n",
    "X = torch.tensor(test_data.iloc[:, :-1].values).to(torch.float32)\n",
    "y = torch.tensor(test_data.iloc[:, -1].values).to(torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract embeddings of the transactions using the GraphSAGE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "f1_value = 0.0\n",
    "with torch.no_grad():\n",
    "    test_embeddings = model(\n",
    "        X.to(device), torch.tensor([[], []], dtype=torch.int).to(device), return_hidden=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate the XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f1, recall, precision, accuracy, conf_mat = evaluate_xgboost(bst, test_embeddings, y)\n",
    "\n",
    "print(f\"\\nXGBoost Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print('Confusion Matrix:', conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copyright and License\n",
    "<hr/>\n",
    "Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.\n",
    "\n",
    "<br/>\n",
    "\n",
    " Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    " you may not use this file except in compliance with the License.\n",
    " You may obtain a copy of the License at\n",
    " \n",
    " http://www.apache.org/licenses/LICENSE-2.0\n",
    " \n",
    " Unless required by applicable law or agreed to in writing, software\n",
    " distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    " WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    " See the License for the specific language governing permissions and\n",
    " limitations under the License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simple_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
