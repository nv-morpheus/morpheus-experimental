{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on TabFormer Data\n",
    "This notebook loads a pre-trained GNN (GraphSAGE) model and an XGBoost model and runs inference on raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals\n",
    "* Outline the steps to transform new raw data before feeding it into the models.\n",
    "* Simulate the use of trained models on new data during inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cudf\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import os\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Path to the pre-trained GraphSAGE and the XGBoost models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_base_path = '../data/TabFormer'\n",
    "model_root_dir = os.path.join(dataset_base_path, 'models')\n",
    "gnn_model_path = os.path.join(model_root_dir, 'node_embedder.pth')\n",
    "xgb_model_path = os.path.join(model_root_dir, 'embedding_based_xgb_model.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of the trained GraphSAGE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, n_hops, dropout_prob=0.25):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "\n",
    "        # list of conv layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        # add first conv layer to the list\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        # add the remaining conv layers to the list\n",
    "        for _ in range(n_hops - 1):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        \n",
    "        # output layer\n",
    "        self.fc = nn.Linear(hidden_channels, out_channels)        \n",
    "\n",
    "    def forward(self, x, edge_index, return_hidden=False):\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "            \n",
    "        if return_hidden:\n",
    "            return x\n",
    "        else:\n",
    "            return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "### Load the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the pre-trained GraphSAGE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GNN model for generating node embeddings\n",
    "gnn_model = torch.load(gnn_model_path)\n",
    "gnn_model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the pre-trained XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load xgboost model for node classification\n",
    "loaded_bst = xgb.Booster()\n",
    "loaded_bst.load_model(xgb_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function to evaluate the XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import cupy as cp\n",
    "from torch.utils.dlpack import to_dlpack\n",
    "\n",
    "def evaluate_xgboost(bst, embeddings, labels):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of the XGBoost model by calculating different metrics.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    bst : xgboost.Booster\n",
    "        The trained XGBoost model to be evaluated.\n",
    "    embeddings : torch.Tensor\n",
    "        The input feature embeddings for transaction nodes.\n",
    "    labels : torch.Tensor\n",
    "        The target labels (Fraud or Non-fraud) transaction, with the same length as the number of \n",
    "        rows in `embeddings`.\n",
    "    Returns:\n",
    "    -------\n",
    "     Confusion matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert embeddings to cuDF DataFrame\n",
    "    embeddings_cudf = cudf.DataFrame(cp.from_dlpack(to_dlpack(embeddings)))\n",
    "    \n",
    "    # Create DMatrix for the test embeddings\n",
    "    dtest = xgb.DMatrix(embeddings_cudf)\n",
    "    \n",
    "    # Predict using XGBoost on GPU\n",
    "    preds = bst.predict(dtest)\n",
    "    pred_labels = (preds > 0.5).astype(int)\n",
    "\n",
    "    # Move labels to CPU for evaluation\n",
    "    labels_cpu = labels.cpu().numpy()\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    accuracy = accuracy_score(labels_cpu, pred_labels)\n",
    "    precision = precision_score(labels_cpu, pred_labels, zero_division=0)\n",
    "    recall = recall_score(labels_cpu, pred_labels, zero_division=0)\n",
    "    f1 = f1_score(labels_cpu, pred_labels, zero_division=0)\n",
    "    roc_auc = roc_auc_score(labels_cpu, preds)\n",
    "\n",
    "    print(f\"Performance of XGBoost model trained on node embeddings\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    conf_mat = confusion_matrix(labels.cpu().numpy(), pred_labels)\n",
    "    print('Confusion Matrix:', conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Evaluate the XGBoost model on untransformed test data (saved in the preprocessing notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read untransformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('future.no_silent_downcasting', True)    \n",
    "path_to_untransformed_data = os.path.join(dataset_base_path, 'xgb', 'untransformed_test.csv')\n",
    "untransformed_df = pd.read_csv(path_to_untransformed_data)\n",
    "untransformed_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data transformer and transform the data using the loaded transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dataset_base_path, 'preprocessor.pkl'),'rb') as f:\n",
    "    loaded_transformer = pickle.load(f)\n",
    "    transformed_data = loaded_transformer.transform(untransformed_df.loc[:, untransformed_df.columns[:-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate the model on the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Convert data to torch tensors\n",
    "X = torch.tensor(transformed_data).to(torch.float32).to(device)\n",
    "y = torch.tensor(untransformed_df[untransformed_df.columns[-1]].values ).to(torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate node embedding using the GNN model\n",
    "test_embeddings = gnn_model(\n",
    "    X.to(device), torch.tensor([[], []], dtype=torch.int).to(device), return_hidden=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the XGBoost model\n",
    "evaluate_xgboost(loaded_bst, test_embeddings, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Predictions on raw input\n",
    "The purpose is to demonstrate the use of the models during inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read example raw inputs\n",
    "raw_file_path = os.path.join(dataset_base_path, 'xgb', 'example_transactions.csv')\n",
    "data = pd.read_csv(raw_file_path)\n",
    "data = data[data.columns[:-1]]\n",
    "original_data = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform raw data\n",
    "* Perform the same set of transformations on the raw data as was done on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename columns before the data is fed into the pre-fitted data transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns before the data is fed into the data transformer\n",
    "COL_USER = 'User'\n",
    "COL_CARD = 'Card'\n",
    "COL_AMOUNT = 'Amount'\n",
    "COL_MCC = 'MCC'\n",
    "COL_TIME = 'Time'\n",
    "COL_DAY = 'Day'\n",
    "COL_MONTH = 'Month'\n",
    "COL_YEAR = 'Year'\n",
    "\n",
    "COL_MERCHANT = 'Merchant'\n",
    "COL_STATE ='State'\n",
    "COL_CITY ='City'\n",
    "COL_ZIP = 'Zip'\n",
    "COL_ERROR = 'Errors'\n",
    "COL_CHIP = 'Chip'\n",
    "\n",
    "\n",
    "_ = data.rename(columns={\n",
    "    \"Merchant Name\": COL_MERCHANT,\n",
    "    \"Merchant State\": COL_STATE,\n",
    "    \"Merchant City\": COL_CITY,\n",
    "    \"Errors?\": COL_ERROR,\n",
    "    \"Use Chip\": COL_CHIP\n",
    "    },\n",
    "    inplace=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle unknown values as was done for the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNKNOWN_STRING_MARKER = 'XX'\n",
    "UNKNOWN_ZIP_CODE = 0\n",
    "MAX_NR_CARDS_PER_YEAR = 9\n",
    "\n",
    "data[COL_STATE] = data[COL_STATE].fillna(UNKNOWN_STRING_MARKER)\n",
    "data[COL_ERROR] = data[COL_ERROR].fillna(UNKNOWN_STRING_MARKER)\n",
    "data[COL_ZIP] = data[COL_ZIP].fillna(UNKNOWN_ZIP_CODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert column type and remove \"$\" and \",\" as was done for the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data[COL_AMOUNT] = data[COL_AMOUNT].str.replace(\"$\",\"\").astype(\"float\")\n",
    "data[COL_STATE] = data[COL_STATE].astype('str')\n",
    "data[COL_MERCHANT] = data[COL_MERCHANT].astype('str')\n",
    "data[COL_ERROR] = data[COL_ERROR].str.replace(\",\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine User and Card to generate unique numbers as was done for the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data[COL_CARD] = data[COL_USER] * MAX_NR_CARDS_PER_YEAR  + data[COL_CARD]\n",
    "data[COL_CARD] = data[COL_CARD].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check if the transactions have unknown users or merchants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the known merchants and (users, cards), i.e. the merchants and (users, cards) that are in training data\n",
    "known_merchants = set()\n",
    "known_cards = set()\n",
    "\n",
    "for enc in  loaded_transformer.named_transformers_['binary'].named_steps['binary'].ordinal_encoder.mapping:\n",
    "    if enc['col'] == COL_MERCHANT:\n",
    "        known_merchants = set(enc['mapping'].keys())\n",
    "    if enc['col'] == COL_CARD:\n",
    "        known_cards = set(enc['mapping'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is user, card already known\n",
    "data['Is_card_known'] = data[COL_CARD].map(lambda c: c in known_cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is merchant already known\n",
    "data['Is_merchant_known'] = data[COL_MERCHANT].map(lambda m: m in known_merchants )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the same set of predictor columns as used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_predictors = [COL_AMOUNT]\n",
    "nominal_predictors = [COL_ERROR, COL_CARD, COL_CHIP, COL_CITY, COL_ZIP, COL_MCC, COL_MERCHANT]\n",
    "\n",
    "predictor_columns = numerical_predictors + nominal_predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the data transformer and transform the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dataset_base_path, 'preprocessor.pkl'),'rb') as f:\n",
    "    loaded_transformer = pickle.load(f)\n",
    "    transformed_data = loaded_transformer.transform(data[predictor_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to GPU if available, otherwise default to CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Convert data to torch tensors\n",
    "X = torch.tensor(transformed_data).to(torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate node embedding using the GraphSAGE model\n",
    "transaction_embeddings = gnn_model(\n",
    "    X.to(device), torch.tensor([[], []], dtype=torch.int).to(device), return_hidden=True)\n",
    "\n",
    "embeddings_cudf = cudf.DataFrame(cp.from_dlpack(to_dlpack(transaction_embeddings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict if the transaction(s) are fraud\n",
    "preds = loaded_bst.predict(xgb.DMatrix(embeddings_cudf))\n",
    "pred_labels = (preds > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If the transactions have unknown (user, card) or merchant, mark it as fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the target column\n",
    "target_col_name = 'Is Fraud?'\n",
    "\n",
    "data[target_col_name] = pred_labels\n",
    "data[target_col_name] = data.apply(\n",
    "    lambda row: \n",
    "    (row[target_col_name] == 1) or (row['Is_card_known'] == False) or (row['Is_merchant_known'] == False), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label the raw data as Fraud or Non-Fraud, based on prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Change 0 to No (non-Fraud) and 1 to Yes (Fraud)\n",
    "binary_to_fraud = { False: 'No', True : 'Yes'}\n",
    "data[target_col_name] = data[target_col_name].map(binary_to_fraud).astype('str')\n",
    "original_data[target_col_name] = data[target_col_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw data with predicted labels (Fraud or Non-Fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copyright and License\n",
    "<hr/>\n",
    "Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.\n",
    "\n",
    "<br/>\n",
    "\n",
    " Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    " you may not use this file except in compliance with the License.\n",
    " You may obtain a copy of the License at\n",
    " \n",
    " http://www.apache.org/licenses/LICENSE-2.0\n",
    " \n",
    " Unless required by applicable law or agreed to in writing, software\n",
    " distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    " WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    " See the License for the specific language governing permissions and\n",
    " limitations under the License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
