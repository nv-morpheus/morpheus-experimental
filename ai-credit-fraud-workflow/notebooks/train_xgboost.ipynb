{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an XGBoost model\n",
    "#### Goals\n",
    "\n",
    "* Build only an XGBoost model without leveraging a GNN.\n",
    "* Establish a baseline performance using the XGBoost model.\n",
    "\n",
    "__NOTE__: This XGBoost model does not leverage embeddings from the GNN (GraphSAGE) model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the datasets to choose from\n",
    "TABFORMER = \"TabFormer\"\n",
    "SPARKOV = \"Sparkov\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the dataset to train the model on\n",
    "__Note__:  This notebook works for both __TabFormer__ and __Sparkov__ dataset. \n",
    "Make sure that the right dataset is selected.\n",
    "For yhe TabFormer dataset, set\n",
    "\n",
    "```code\n",
    "    DATASET = TABFORMER\n",
    "```\n",
    "and for the Sparkov dataset, set\n",
    "\n",
    "```code\n",
    "    DATASET = SPARKOV\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to either TABFORMER or SPARKOV\n",
    "DATASET = TABFORMER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries, packages, and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import cudf\n",
    "import cupy\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc, f1_score, precision_score, recall_score\n",
    "\n",
    "from cuml.metrics import confusion_matrix, precision_recall_curve, roc_auc_score\n",
    "from cuml.metrics.accuracy import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Path to pre-processed data and directory to save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateset_name_to_path= defaultdict(lambda: \"../data/TabFormer\")\n",
    "\n",
    "dateset_name_to_path['TabFormer'] = '../data/TabFormer'\n",
    "dateset_name_to_path['Sparkov'] = '../data/Sparkov'\n",
    "dataset_dir = dateset_name_to_path[DATASET]\n",
    "xgb_data_dir = os.path.join(dataset_dir, 'xgb')\n",
    "models_dir = os.path.join(dataset_dir, 'models')\n",
    "model_file_name = 'xgboost_model.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load and prepare training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data_path = os.path.join(xgb_data_dir, \"training.csv\")\n",
    "df = cudf.read_csv(train_data_path)\n",
    "\n",
    "# Target column\n",
    "target_col_name = df.columns[-1]\n",
    "\n",
    "# Split the dataframe into features (X) and labels (y)\n",
    "y = df[target_col_name]\n",
    "X = df.drop(target_col_name, axis=1)\n",
    "\n",
    "# Split data into trainand testing sets\n",
    "from cuml.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Convert the training and test data to DMatrix\n",
    "dtrain = xgb.DMatrix(data=X_train, label=y_train)\n",
    "deval = xgb.DMatrix(data=X_val, label=y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter grid to search for the best hyper-parameters for the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Define the parameter grid for manual search\n",
    "param_grid = {\n",
    "    'max_depth': [5, 6],\n",
    "    'learning_rate': [0.3, 0.4, 0.45],\n",
    "    'n_estimators': [100, 150],\n",
    "    'gamma': [0, 0.1],\n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "param_combinations = list(itertools.product(*param_grid.values()))\n",
    "\n",
    "# Print all combinations of hyperparameters (optional)\n",
    "print(\"Total number of parameter combinations:\", len(param_combinations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search for the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = float(\"inf\")  # Initialize best score\n",
    "best_params = None  # To store best hyperparameters\n",
    "\n",
    "for params_comb in param_combinations:\n",
    "    \n",
    "    # Create a dictionary of parameters\n",
    "    params = {\n",
    "        'max_depth': params_comb[0],\n",
    "        'learning_rate': params_comb[1],\n",
    "        'gamma': params_comb[3],\n",
    "        'eval_metric': 'logloss',\n",
    "        'objective': 'binary:logistic',  # For binary classification\n",
    "        'tree_method': 'hist',  # GPU support\n",
    "        'device': 'cuda'\n",
    "    }\n",
    "\n",
    "    # Train the model using xgb.train and the Booster\n",
    "    evals = [(dtrain, 'train'), (deval, 'eval')]\n",
    "    bst = xgb.train(params, dtrain, num_boost_round=params_comb[2], evals=evals, \n",
    "                    early_stopping_rounds=10, verbose_eval=False)\n",
    "    \n",
    "    # Get the evaluation score (logloss) on the validation set\n",
    "    score = bst.best_score  # The logloss score (or use other eval_metric)\n",
    "\n",
    "    # Update the best parameters if the current model is better\n",
    "    if score < best_score:\n",
    "        best_score = score\n",
    "        best_params = params\n",
    "        best_num_boost_round = bst.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params, best_score, best_num_boost_round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model with the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the final model using the best parameters and best number of boosting rounds\n",
    "dtrain = xgb.DMatrix(data=X, label=y)\n",
    "final_model = xgb.train(best_params, dtrain, num_boost_round=best_num_boost_round)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the best model\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "final_model.save_model(os.path.join(models_dir, model_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Evaluate the model on the same unseen data that is used for testing GNN based XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the model from the file\n",
    "best_model_loaded = xgb.Booster()\n",
    "best_model_loaded.load_model(os.path.join(models_dir, model_file_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and prepare unseen test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_data_path = os.path.join(xgb_data_dir, \"test.csv\")\n",
    "\n",
    "test_df = cudf.read_csv(test_data_path)\n",
    "\n",
    "dnew = xgb.DMatrix(test_df.drop(target_col_name, axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions\n",
    "y_pred_prob = best_model_loaded.predict(dnew)\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute metrics to evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test = test_df[target_col_name].values \n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(conf_mat)\n",
    "\n",
    "# ROC AUC Score\n",
    "r_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(f'ROC AUC Score: {r_auc:.4f}')\n",
    "\n",
    "y_test = cupy.asnumpy(y_test)\n",
    "# Precision\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(f'Precision: {precision:.4f}')\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(f'Recall: {recall:.4f}')\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f'F1 Score: {f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Precision-Recall curve\n",
    "* A Precision-Recall Curve shows the trade-off between precision and recall for a model at various thresholds, helping assess performance, especially on imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute Precision, Recall, and thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Compute the Area Under the Curve (AUC) for Precision-Recall\n",
    "pr_auc = auc(recall, precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot precision-recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.2f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot precision-recall curve with thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(thresholds, precision[:-1], label=\"Precision\")\n",
    "plt.plot(thresholds, recall[:-1], label=\"Recall\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Precision-Recall Curve with Thresholds\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One can choose optimal threshold based on the F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copyright and License\n",
    "<hr/>\n",
    "Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.\n",
    "\n",
    "<br/>\n",
    "\n",
    " Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    " you may not use this file except in compliance with the License.\n",
    " You may obtain a copy of the License at\n",
    " \n",
    " http://www.apache.org/licenses/LICENSE-2.0\n",
    " \n",
    " Unless required by applicable law or agreed to in writing, software\n",
    " distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    " WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    " See the License for the specific language governing permissions and\n",
    " limitations under the License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
