{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c6a5b09-a601-47c6-989f-5efb42d7f4f8",
   "metadata": {},
   "source": [
    "# Credit Card Transaction Data Cleanup and Prep \n",
    "\n",
    "This notebook shows the steps for cleanup and preparing the credit card transaction data for follow on GNN training with GraphSAGE.\n",
    "\n",
    "### The dataset:\n",
    " * 'Generate Fake Credit Card Transaction [Data](https://www.kaggle.com/datasets/kartik2112/fraud-detection), Including Fraudulent Transactions' using https://github.com/namebrandon/Sparkov_Data_Generation\n",
    " * Released under CC0: Public Domain\n",
    "\n",
    "Contains 1,296,675 records with 15 fields, one field being the \"is fraud\" label which we use for training.\n",
    "\n",
    "### Goals\n",
    "The goal is to:\n",
    " * Understand and transform the data\n",
    "   * Correlation analysis to select important predictors \n",
    "   * Encode categorical fields\n",
    "   * Scale numerical columns\n",
    "   * Create a continuous node index across users, merchants, and transactions\n",
    "     * having node ID start at zero and then be contiguous is critical for creation of Compressed Sparse Row (CSR) formatted data without wasting memory.\n",
    " * Produce:\n",
    "   * For XGBoost:\n",
    "     * Training   - all transactions in 2019\n",
    "     * Validation - all transactions between January and May in 2020\n",
    "     * Test.      - all transactions after May 2020\n",
    "   * For GNN\n",
    "     * Training Data \n",
    "       * Edge List \n",
    "       * Feature data\n",
    "   * Test set - all transactions after May 2020\n",
    "\n",
    "\n",
    "\n",
    "### Graph formation\n",
    "Given that we are limited to just the data in the transaction file, the ideal model would be to have a bipartite graph of Users to Merchants where the edges represent the credit card transaction and then perform Link Classification on the Edges to identify fraud. Unfortunately the current version of cuGraph does not support GNN Link Prediction. That limitation will be lifted over the next few release at which time this code will be updated. Luckily, there is precedence for viewing transactions as nodes and then doing node classification using the popular GraphSAGE GNN. That is the approach this code takes. The produced graph will be a tri-partite graph where each transaction is represented as a node.\n",
    "\n",
    "<img src=\"../img/3-partite.jpg\" width=\"35%\"/>\n",
    "\n",
    "\n",
    "### Features\n",
    "For the XGBoost approach, there is no need to generate empty features for the Merchants. However, for GNN processing, every node needs to have the same set of feature data. Therefore, we need to generate empty features for the User and Merchant nodes. \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795bdece",
   "metadata": {},
   "source": [
    "#### Import the necessary libraries.  In this case will be use cuDF and perform most of the data prep in GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b6b2bc6-a206-42c5-aae9-590672b3a202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import cudf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as ss\n",
    "from category_encoders import BinaryEncoder\n",
    "from scipy.stats import gaussian_kde, pointbiserialr\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81db641b",
   "metadata": {},
   "source": [
    "-------\n",
    "#### Define some arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "016964ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether the graph is undirected\n",
    "make_undirected = True\n",
    "\n",
    "# Whether to spread features across Users and Merchants nodes\n",
    "spread_features = False\n",
    "\n",
    "# Whether we should under-sample majority class (i.e. non-fraud transactions)\n",
    "under_sample = True\n",
    "\n",
    "# Ration of fraud and non-fraud transactions in case we under-sample the majority class\n",
    "fraud_ratio = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "656e6aee-038a-4b58-9296-993e06defb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkov_base_path = '../data/Sparkov'\n",
    "sparkov_raw_file_path = os.path.join(sparkov_base_path, 'raw', 'fraudTrain.csv')\n",
    "sparkov_xgb = os.path.join(sparkov_base_path, 'xgb')\n",
    "sparkov_gnn = os.path.join(sparkov_base_path, 'gnn')\n",
    "if not os.path.exists(sparkov_xgb):\n",
    "    os.makedirs(sparkov_xgb)\n",
    "if not os.path.exists(sparkov_gnn):\n",
    "    os.makedirs(sparkov_gnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fe43fe",
   "metadata": {},
   "source": [
    "--------\n",
    "## Load and understand the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb41e6ea-1e9f-4f14-99a4-d6d3df092a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "data = cudf.read_csv(sparkov_raw_file_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9a9ab4-4240-4824-997b-8bfdb640381c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional - take a look at the data \n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cb6dff",
   "metadata": {},
   "source": [
    "### Findings\n",
    "* Nominal categorical fields - 'cc_num', 'merchant', 'category', 'first', 'last', 'street', 'city', 'state', 'zip', 'job', 'trans_num'\n",
    "* Numerical fields - 'amt', 'lat', 'long', 'city_pop', 'merch_lat', 'merch_long'\n",
    "* Timestamp fields - 'dob', 'trans_date_trans_time', 'unix_time'\n",
    "* Target label - 'is_fraud'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c3ed53",
   "metadata": {},
   "source": [
    "#### How many transactions are fraud?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3a9ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['is_fraud'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d74fa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of fraud transactions\n",
    "100.0*(data['is_fraud'] == 1).sum()/len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd28023",
   "metadata": {},
   "source": [
    "##### Findings - The dataset is extremely imbalanced, only 0.58% of the transactions are fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39b1a60",
   "metadata": {},
   "source": [
    "#### Check if are there Null values in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2154e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any column has missing values\n",
    "data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cbbd84",
   "metadata": {},
   "source": [
    "###### Great, none of the columns have null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf29e24",
   "metadata": {},
   "source": [
    "##### Save a few transactions before any operations on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92df5856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a few raw transactions for model's inference notebook\n",
    "out_path = os.path.join(sparkov_xgb, 'example_transactions.csv')\n",
    "data.tail(10).to_pandas().to_csv(out_path, header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa7fc88",
   "metadata": {},
   "source": [
    "#### Convert 'dob' to 'age' w.r.t. a reference date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4df4100",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['dob'] = cudf.to_datetime(data['dob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b55a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "one_nanosecond = np.timedelta64(1, 'ns')\n",
    "nanoseconds_in_year = 365.25 * 24 * 60 * 60 * 1e9\n",
    "reference_date =  cudf.to_datetime('2024-10-30') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "034210a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['age'] = data['dob'].apply(lambda dob: (reference_date - dob)/ one_nanosecond / nanoseconds_in_year )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a72a46a",
   "metadata": {},
   "source": [
    "#### Split transaction time in year, month, day and time where time indicate number of minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bc3ec47",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_date_time =  cudf.to_datetime(data.trans_date_trans_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0f61401",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['year'] = tx_date_time.dt.year\n",
    "data['month'] = tx_date_time.dt.month\n",
    "data['day'] = tx_date_time.dt.day\n",
    "data['time'] = tx_date_time.dt.hour*60 +  tx_date_time.dt.minute\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f372c0",
   "metadata": {},
   "source": [
    "##### Observations\n",
    "\n",
    "* we can treat 'year', 'month', 'day' as ordinal fields and time as numerical field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb202e2",
   "metadata": {},
   "source": [
    "### From ('lat', 'long'), ('merchant_lat', 'merchant_long') and unix_time compute transaction speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90714a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp_df = pd.DataFrame()\n",
    "\n",
    "# Haversine formula function\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Radius of Earth in km\n",
    "    R = 6371.0\n",
    "\n",
    "    # Convert degrees to radians\n",
    "    lat1 = math.radians(lat1)\n",
    "    lon1 = math.radians(lon1)\n",
    "    lat2 = math.radians(lat2)\n",
    "    lon2 = math.radians(lon2)\n",
    "\n",
    "    # Differences in coordinates\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    # Haversine formula\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "    # Distance in kilometers\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "temp_df=  data[['unix_time', 'lat', 'long', 'merch_lat', 'merch_long']].to_pandas()\n",
    "temp_df['tx_duration'] = temp_df['unix_time'].apply(lambda x: x/1e9)\n",
    "temp_df['distance_km'] = temp_df.apply(\n",
    "    lambda row: haversine(row['lat'], row['long'], row['merch_lat'], row['merch_long']), axis=1)\n",
    "data['speed'] =  (temp_df['distance_km']/temp_df['tx_duration'])\n",
    "del temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5805e8",
   "metadata": {},
   "source": [
    "#### Using variables for makes code cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c933e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "COL_CARD = 'cc_num'\n",
    "COL_MCC = 'category'\n",
    "COL_MERCHANT = 'merchant'\n",
    "COL_STATE ='state'\n",
    "COL_CITY ='city'\n",
    "COL_ZIP = 'zip'\n",
    "\n",
    "COL_AMOUNT = 'amt'\n",
    "COL_CITY_POP = 'city_pop'\n",
    "\n",
    "COL_FRAUD = 'is_fraud'\n",
    "\n",
    "COL_TIME = 'time'\n",
    "COL_DAY = 'day'\n",
    "COL_MONTH = 'month'\n",
    "COL_YEAR = 'year'\n",
    "COL_AGE = 'age'\n",
    "COL_JOB = 'job'\n",
    "COL_SPEED = 'speed'\n",
    "\n",
    "NUMERICAL_COLUMNS = [\n",
    "    COL_AMOUNT, COL_CITY_POP, COL_TIME, COL_AGE, COL_SPEED,\n",
    "    'lat', 'long', 'merch_lat', 'merch_long' ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128c578e",
   "metadata": {},
   "source": [
    "##### Number of cards per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8435e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.cc_num.unique()) / len((data['first'] + data['last']).unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9c8002",
   "metadata": {},
   "source": [
    "#### Look into numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924c30fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[NUMERICAL_COLUMNS].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73172495",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "* 'amt' and 'city_pop' have extreme values or outliers compared to mean and median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ae0373",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[COL_AMOUNT].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92cd79e",
   "metadata": {},
   "source": [
    "##### Plot histogram of the 'amt' field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f1809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "kde = gaussian_kde(data[COL_AMOUNT].to_pandas())\n",
    "x_vals = np.linspace(data[COL_AMOUNT].min(), 2000, 100)\n",
    "plt.plot(x_vals, kde(x_vals), color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867ea0d4",
   "metadata": {},
   "source": [
    "##### Findings\n",
    "* very few transactions have higher 'amt' values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2322497",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[COL_CITY_POP].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01865293",
   "metadata": {},
   "source": [
    "##### Plot histogram of the 'city_pop' field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f5c519",
   "metadata": {},
   "outputs": [],
   "source": [
    "kde = gaussian_kde(data[COL_CITY_POP].to_pandas())\n",
    "x_vals = np.linspace(data[COL_CITY_POP].min(), 100000, 100)\n",
    "plt.plot(x_vals, kde(x_vals), color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d3941c",
   "metadata": {},
   "source": [
    "##### Findings\n",
    "* Only a few cities have a population over 40,000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab82a9f9",
   "metadata": {},
   "source": [
    "#### Let's look into how the amount differ between fraud and non-fraud transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31679a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[COL_AMOUNT].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66de8459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraud transactions\n",
    "data[COL_AMOUNT][data[COL_FRAUD] == 1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f49ff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-fraud transactions\n",
    "data[COL_AMOUNT][data[COL_FRAUD] == 0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1151a313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-fraud transactions with high value of amount \n",
    "data[COL_AMOUNT] [ (data[COL_FRAUD]==0) & (data[COL_AMOUNT] > 1376)  ].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a190bb9e",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "\n",
    "* Average amount in fraud transactions > 8x the average amount in non-fraud transactions\n",
    "* Interestingly, many non-fraud transactions have high amount as well.\n",
    "\n",
    "We need to scale the data, and RobustScaler could be a good choice for it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f56d5a-f135-4af2-ba13-b926f66a045f",
   "metadata": {},
   "source": [
    "#### Number of unique values per nominal columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dff40cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many unique values for \n",
    "for col in [COL_STATE, COL_CITY, COL_ZIP, COL_MERCHANT, COL_MCC, COL_CARD]:\n",
    "    print(f'#unique values ({col}) = {len(data[col].unique())}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ca593a",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "We can binary encode 'state', 'city', 'zip', 'merchant', 'category', 'cc_num', if the columns have good correlation with targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50933790-780c-43cc-833d-c7ad16acbde3",
   "metadata": {},
   "source": [
    "#### Take a look into distribution of 'time', 'speed' and 'age' columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691a355b",
   "metadata": {},
   "source": [
    "##### Plot histogram of transaction 'speed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d490cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "kde = gaussian_kde(data[COL_SPEED].to_pandas())\n",
    "x_vals = np.linspace(data[COL_SPEED].min(), data[COL_SPEED].max(), 100)\n",
    "plt.plot(x_vals, kde(x_vals), color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e1d50f",
   "metadata": {},
   "source": [
    "##### Plot histogram of 'time'\n",
    "__NOTE__ Time is captured as number of minutes over the span of a day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd994bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "kde = gaussian_kde(data[COL_TIME].to_pandas())\n",
    "x_vals = np.linspace(data[COL_TIME].min(), data[COL_TIME].max(), 100)\n",
    "plt.plot(x_vals, kde(x_vals), color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97873360",
   "metadata": {},
   "source": [
    "##### Plot histogram of 'age'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576fb419",
   "metadata": {},
   "outputs": [],
   "source": [
    "kde = gaussian_kde(data[COL_AGE].to_pandas())\n",
    "x_vals = np.linspace(data[COL_AGE].min(), data[COL_AGE].max(), 100)\n",
    "plt.plot(x_vals, kde(x_vals), color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75c7c59",
   "metadata": {},
   "source": [
    "##### Findings\n",
    "* It's not obvious from the histogram of 'time,' 'speed,' and 'age' whether they are clear indicators for labeling a transaction as fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a815bc9",
   "metadata": {},
   "source": [
    "#### Define a function to compute correlation of different categorical fields with target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cfaa31fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/Cram%C3%A9r's_V\n",
    "\n",
    "def cramers_v(x, y):\n",
    "    confusion_matrix = cudf.crosstab(x, y).to_numpy()\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    r, k = confusion_matrix.shape\n",
    "    return np.sqrt(chi2 / (n * (min(k-1, r-1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa39773",
   "metadata": {},
   "source": [
    "##### Compute correlation of different field with target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5497f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_factor = 1\n",
    "columns_to_compute_corr =  [\n",
    "    COL_CARD, COL_STATE, COL_CITY, COL_ZIP, COL_MCC, COL_MERCHANT,\n",
    "    COL_DAY, COL_MONTH, COL_YEAR, COL_JOB, 'gender']\n",
    "for c1 in columns_to_compute_corr:\n",
    "    for c2 in [COL_FRAUD]:\n",
    "        coff =  100 * cramers_v(data[c1][::sparse_factor], data[c2][::sparse_factor])\n",
    "        print('Correlation ({}, {}) = {:6.2f}%'.format(c1, c2, coff))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738cd723",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "* 'day', 'month', and 'year' 'gender' are not important to predict if a transaction is fraud or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00296660",
   "metadata": {},
   "source": [
    "#### Check how City, State and Zip are correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac25d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sparse_factor = 1\n",
    "columns_to_compute_corr =  [COL_STATE, COL_CITY, COL_ZIP]\n",
    "for c1 in columns_to_compute_corr:\n",
    "    for c2 in columns_to_compute_corr:\n",
    "        if c1 not in c2:\n",
    "            coff =  100 * cramers_v(data[c1][::sparse_factor], data[c2][::sparse_factor])\n",
    "            print('{} {} {:6.2f}%'.format(c1, c2, coff))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b1edd3",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "* if we use 'zip' to predict if a transaction is fraud or not, we don't need to use 'city' and 'state'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50d67c2",
   "metadata": {},
   "source": [
    "### Correlation of target with numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38353361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/Point-biserial_correlation_coefficient\n",
    "# Use Point-biserial correlation coefficient(rpb) to check if the numerical columns are important to predict if a transaction is fraud\n",
    "\n",
    "for col in NUMERICAL_COLUMNS:\n",
    "    r_pb, p_value = pointbiserialr(data[COL_FRAUD].to_pandas(), data[col].to_pandas())\n",
    "    print('r_pb ({}) = {:3.2f} with p_value {:3.2f}'.format(col,  r_pb, p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400c8f83",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "* 'amt' column has positive correlation with target\n",
    "* other columns, such as 'city_pop', 'time', 'age', 'lat', 'long', 'merch_lat', and  'merch_long' has negligible correlation with target\n",
    "* Speed can't be ignored as the p_value > 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92d58ef",
   "metadata": {},
   "source": [
    "#### Based on correlation values, select a set of columns (aka fields) to predict whether a transaction is fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a00a7813",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numerical_predictors = [COL_AMOUNT, COL_SPEED, COL_AGE]\n",
    "nominal_predictors = [COL_CARD, COL_ZIP, COL_MCC, COL_MERCHANT, COL_JOB]\n",
    "\n",
    "predictor_columns = numerical_predictors + nominal_predictors\n",
    "\n",
    "target_column = [COL_FRAUD]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9341f5e0",
   "metadata": {},
   "source": [
    "#### Remove duplicates non-fraud data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428c4ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates data points\n",
    "fraud_data = data[data[COL_FRAUD] == 1]\n",
    "data = data[data[COL_FRAUD] == 0]\n",
    "data = data.drop_duplicates(subset=nominal_predictors)\n",
    "data = cudf.concat([data, fraud_data])\n",
    "\n",
    "100*data[COL_FRAUD].value_counts()/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe02370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portion of fraud and non-fraud cases\n",
    "data[COL_YEAR].value_counts()/len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc2ebac",
   "metadata": {},
   "source": [
    "### Split data\n",
    "All the transactions were made in year 2019. Let's split the data into three groups based on event month\n",
    "* Training   - all transactions in 2019\n",
    "* Validation - all transactions between January and May in 2020\n",
    "* Test.      - all transactions after May 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dc748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if under_sample:    \n",
    "    fraud_df = data[data[COL_FRAUD]==1]\n",
    "    non_fraud_df = data[data[COL_FRAUD]==0]\n",
    "    nr_non_fraud_samples = min((len(data) - len(fraud_df)), int(len(fraud_df)/fraud_ratio))\n",
    "    data = cudf.concat([fraud_df, non_fraud_df.sample(nr_non_fraud_samples)])\n",
    "\n",
    "training_idx = data[COL_YEAR] == 2019\n",
    "validation_idx = (data[COL_YEAR] == 2020)  & (data[COL_MONTH] < 4 )\n",
    "test_idx = (data[COL_YEAR] == 2020)  & (data[COL_MONTH] >= 4 )\n",
    "\n",
    "data[COL_FRAUD].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7d2f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# portion of data for training, test and validation\n",
    "training_idx.sum()/len(data), validation_idx.sum()/len(data), test_idx.sum()/len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd036929",
   "metadata": {},
   "source": [
    "### Scale numerical columns and encode categorical columns of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2d570cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As some of the encoder we want to use is not available in cuml yet, we can use pandas for now.\n",
    "# Move training data to pandas for preprocessing\n",
    "pdf_training = data[training_idx].to_pandas()[predictor_columns + target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d135cc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use binary encoding for categorical columns\n",
    "columns_for_binary_encoding = nominal_predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee6b77fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark categorical column as \"category\"\n",
    "pdf_training[nominal_predictors] = pdf_training[nominal_predictors].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6d20de67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "49eb57f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoders to encode categorical columns and scalers to scale numerical columns\n",
    "\n",
    "bin_encoder = Pipeline(\n",
    "    steps=[\n",
    "        (\"binary\", BinaryEncoder(handle_missing='value', handle_unknown='value'))\n",
    "    ]\n",
    ")\n",
    "onehot_encoder = Pipeline(\n",
    "    steps=[\n",
    "        (\"onehot\", OneHotEncoder())\n",
    "    ]\n",
    ")\n",
    "\n",
    "std_scaler = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"standard\", StandardScaler())],\n",
    ")\n",
    "\n",
    "robust_scaler = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"robust\", RobustScaler())],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "613db861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compose encoders and scalers in a column transformer\n",
    "transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"binary\", bin_encoder, columns_for_binary_encoding ),    \n",
    "        (\"robust\", robust_scaler, [COL_AMOUNT]),\n",
    "        (\"stdscaler\", std_scaler, [COL_SPEED, COL_AGE]),\n",
    "    ], remainder=\"passthrough\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79be6b2b",
   "metadata": {},
   "source": [
    "##### Fit column transformer with training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ba373e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit column transformer with training data\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "transformer = transformer.fit(pdf_training[predictor_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9f10f84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed column names\n",
    "columns_of_transformed_data = list(\n",
    "    map(lambda name: name.split('__')[1],\n",
    "        list(transformer.get_feature_names_out(predictor_columns))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "739f62a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data type of transformed columns \n",
    "type_mapping = {}\n",
    "for col in columns_of_transformed_data:\n",
    "    if col.split('_')[0] in nominal_predictors:\n",
    "        type_mapping[col] = 'int8'\n",
    "    elif col in numerical_predictors:\n",
    "        type_mapping[col] = 'float'\n",
    "    elif col in target_column:\n",
    "        type_mapping[col] = data.dtypes.to_dict()[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b5471b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform training data\n",
    "preprocessed_training_data = transformer.transform(pdf_training[predictor_columns])\n",
    "\n",
    "# Convert transformed data to panda DataFrame\n",
    "preprocessed_training_data = pd.DataFrame(\n",
    "    preprocessed_training_data, columns=columns_of_transformed_data)\n",
    "# Copy target column\n",
    "preprocessed_training_data[COL_FRAUD] = pdf_training[COL_FRAUD].values\n",
    "preprocessed_training_data = preprocessed_training_data.astype(type_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f02184b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the transformer \n",
    "\n",
    "with open(os.path.join(sparkov_base_path, 'preprocessor.pkl'),'wb') as f:\n",
    "    pickle.dump(transformer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3de882",
   "metadata": {},
   "source": [
    "#### Save transformed training data for XGBoost training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e3362673",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(sparkov_base_path, 'preprocessor.pkl'),'rb') as f:\n",
    "    loaded_transformer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "47f6663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform test data using the transformer fitted on training data\n",
    "pdf_test = data[test_idx].to_pandas()[predictor_columns + target_column]\n",
    "pdf_test[nominal_predictors] = pdf_test[nominal_predictors].astype(\"category\")\n",
    "\n",
    "preprocessed_test_data = loaded_transformer.transform(pdf_test[predictor_columns])\n",
    "preprocessed_test_data = pd.DataFrame(preprocessed_test_data, columns=columns_of_transformed_data)\n",
    "\n",
    "# Copy target column\n",
    "preprocessed_test_data[COL_FRAUD] = pdf_test[COL_FRAUD].values\n",
    "preprocessed_test_data = preprocessed_test_data.astype(type_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ce80a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform validation data using the transformer fitted on training data\n",
    "pdf_validation = data[validation_idx].to_pandas()[predictor_columns + target_column]\n",
    "pdf_validation[nominal_predictors] = pdf_validation[nominal_predictors].astype(\"category\")\n",
    "\n",
    "preprocessed_validation_data = loaded_transformer.transform(pdf_validation[predictor_columns])\n",
    "preprocessed_validation_data = pd.DataFrame(preprocessed_validation_data, columns=columns_of_transformed_data)\n",
    "\n",
    "# Copy target column\n",
    "preprocessed_validation_data[COL_FRAUD] = pdf_validation[COL_FRAUD].values\n",
    "preprocessed_validation_data = preprocessed_validation_data.astype(type_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2ca66b-d3dc-4f67-9754-b90bbea6e286",
   "metadata": {},
   "source": [
    "## Write out the data for XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "89c16cfb-0bd4-4efb-a610-f3ae7445d96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training data\n",
    "out_path = os.path.join(sparkov_xgb, 'training.csv')\n",
    "if not os.path.exists(os.path.dirname(out_path)):\n",
    "  os.makedirs(os.path.dirname(out_path))\n",
    "preprocessed_training_data.to_csv(\n",
    "  out_path, header=True, index=False, columns=columns_of_transformed_data + target_column)\n",
    "# preprocessed_training_data.to_parquet(out_path, index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f3ef6b19-062b-42d5-8caa-6f6011648b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## validation data\n",
    "out_path = os.path.join(sparkov_xgb, 'validation.csv')\n",
    "if not os.path.exists(os.path.dirname(out_path)):\n",
    "  os.makedirs(os.path.dirname(out_path))\n",
    "preprocessed_validation_data.to_csv(\n",
    "  out_path, header=True, index=False, columns=columns_of_transformed_data + target_column)\n",
    "# preprocessed_validation_data.to_parquet(out_path, index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cdc8e3b9-841d-49f3-b3ae-3017a04605e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test data\n",
    "out_path = os.path.join(sparkov_xgb, 'test.csv')\n",
    "preprocessed_test_data.to_csv(\n",
    "    out_path, header=True, index=False, columns=columns_of_transformed_data + target_column)\n",
    "# preprocessed_test_data.to_parquet(out_path, index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "67fb32b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write untransformed test data that has only (renamed) predictor columns and target\n",
    "out_path = os.path.join(sparkov_xgb, 'untransformed_test.csv')\n",
    "pdf_test.to_csv(out_path, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2d6cb604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete dataFrames that are not needed anymore\n",
    "del(pdf_training)\n",
    "del(pdf_validation)\n",
    "del(pdf_test)\n",
    "del(preprocessed_training_data)\n",
    "del(preprocessed_validation_data)\n",
    "del(preprocessed_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfbfd83",
   "metadata": {},
   "source": [
    "### GNN Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e518c8",
   "metadata": {},
   "source": [
    "#### Setting Vertex IDs\n",
    "In order to create a graph, the different vertices need to be assigned unique vertex IDs. Additionally, the IDs needs to be consecutive and positive.\n",
    "\n",
    "There are three nodes groups here: Transactions, Users, and Merchants. \n",
    "\n",
    "These IDs are not used in training, just used for graph processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "194a47d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same training data as used for XGBoost\n",
    "data = data[training_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0ba0cb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a lot of process has occurred, sort the data and reset the index\n",
    "data = data.sort_values(by=[COL_YEAR, COL_MONTH, COL_DAY, COL_TIME], ascending=False)\n",
    "data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2a75c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each transaction gets a unique ID\n",
    "COL_TRANSACTION_ID = 'Tx_ID'\n",
    "COL_MERCHANT_ID = 'Merchant_ID'\n",
    "COL_USER_ID = 'User_ID'\n",
    "\n",
    "# The number of transaction is the same as the size of the list, and hence the index value\n",
    "data[COL_TRANSACTION_ID] = data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "472ea57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the max transaction ID to compute first merchant ID\n",
    "max_tx_id = data[COL_TRANSACTION_ID].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8ef04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Merchant string to consecutive integers\n",
    "merchant_name_to_id = dict((v, k) for k, v in data[COL_MERCHANT].unique().to_dict().items())\n",
    "data[COL_MERCHANT_ID] = data[COL_MERCHANT].map(merchant_name_to_id) + (max_tx_id + 1)\n",
    "data[COL_MERCHANT_ID].min(), data[COL_MERCHANT_ID].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6937df18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, get the max merchant ID to compute first user ID\n",
    "max_merchant_id = data[COL_MERCHANT_ID].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63c7fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Card to consecutive user IDs\n",
    "user_id_to_consecutive_ids = dict((v, k) for k, v in data[COL_CARD].unique().to_dict().items())\n",
    "data[COL_USER_ID] = data[COL_CARD].map(user_id_to_consecutive_ids) + max_merchant_id + 1\n",
    "data[COL_USER_ID].min(), data[COL_USER_ID].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "28858422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the max user ID\n",
    "max_user_id = data[COL_USER_ID].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903e5115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the the transaction, merchant and user ids are consecutive\n",
    "id_range = data[COL_TRANSACTION_ID].min(), data[COL_TRANSACTION_ID].max()\n",
    "print(f'Transaction ID range {id_range}')\n",
    "id_range = data[COL_MERCHANT_ID].min(), data[COL_MERCHANT_ID].max()\n",
    "print(f'Merchant ID range {id_range}')\n",
    "id_range = data[COL_USER_ID].min(), data[COL_USER_ID].max()\n",
    "print(f'User ID range {id_range}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f2d0dfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "assert( data[COL_TRANSACTION_ID].max() == data[COL_MERCHANT_ID].min() - 1)\n",
    "assert( data[COL_MERCHANT_ID].max() == data[COL_USER_ID].min() - 1)\n",
    "assert(len(data[COL_USER_ID].unique()) == (data[COL_USER_ID].max() - data[COL_USER_ID].min() + 1))\n",
    "assert(len(data[COL_MERCHANT_ID].unique()) == (data[COL_MERCHANT_ID].max() - data[COL_MERCHANT_ID].min() + 1))\n",
    "assert(len(data[COL_TRANSACTION_ID].unique()) == (data[COL_TRANSACTION_ID].max() - data[COL_TRANSACTION_ID].min() + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9c3df3-a5be-4899-8bf9-6152aca114c7",
   "metadata": {},
   "source": [
    "### Write out the data for GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b86862-d129-4ece-a60d-dc798f3a68b5",
   "metadata": {},
   "source": [
    "#### Create the Graph Edge Data file \n",
    "The file is in COO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b288c5a7-20dd-40ff-b0eb-7a5895bcc464",
   "metadata": {},
   "outputs": [],
   "source": [
    "COL_GRAPH_SRC = 'src'\n",
    "COL_GRAPH_DST = 'dst'\n",
    "COL_GRAPH_WEIGHT = 'wgt'\n",
    "\n",
    "# User to Transactions\n",
    "U_2_T = cudf.DataFrame()\n",
    "U_2_T[COL_GRAPH_SRC] = data[COL_USER_ID]\n",
    "U_2_T[COL_GRAPH_DST] = data[COL_TRANSACTION_ID]\n",
    "if make_undirected:\n",
    "  T_2_U = cudf.DataFrame()\n",
    "  T_2_U[COL_GRAPH_SRC] = data[COL_TRANSACTION_ID]\n",
    "  T_2_U[COL_GRAPH_DST] = data[COL_USER_ID]\n",
    "  U_2_T = cudf.concat([U_2_T, T_2_U])\n",
    "  del T_2_U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a970747d-07a2-43b3-b39c-19a0196fa5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transactions to Merchants\n",
    "T_2_M = cudf.DataFrame()\n",
    "T_2_M[COL_GRAPH_SRC] = data[COL_MERCHANT_ID]\n",
    "T_2_M[COL_GRAPH_DST] = data[COL_TRANSACTION_ID]\n",
    "\n",
    "if make_undirected:\n",
    "  M_2_T = cudf.DataFrame()\n",
    "  M_2_T[COL_GRAPH_SRC] = data[COL_TRANSACTION_ID]\n",
    "  M_2_T[COL_GRAPH_DST] = data[COL_MERCHANT_ID]\n",
    "  T_2_M = cudf.concat([T_2_M, M_2_T])\n",
    "  del M_2_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e704fd-ae9f-45b1-ad56-bdc0b743d09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Edge = cudf.concat([U_2_T, T_2_M])\n",
    "Edge[COL_GRAPH_WEIGHT] = 0.0\n",
    "len(Edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c74572f6-ff6e-4c8f-803e-0ae2c0587c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now write out the data\n",
    "out_path = os.path.join (sparkov_gnn, 'edges.csv')\n",
    "\n",
    "if not os.path.exists(os.path.dirname(out_path)):\n",
    "  os.makedirs(os.path.dirname(out_path))\n",
    "  \n",
    "Edge.to_csv(out_path, header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3dd3ff45-3796-4069-9e3a-587743c4e1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(Edge)\n",
    "del(U_2_T)\n",
    "del(T_2_M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed00c481-1737-4152-9d23-f3cb24f2adcd",
   "metadata": {},
   "source": [
    "### Now the feature data\n",
    "Feature data needs to be is sorted in order, where the row index corresponds to the node ID\n",
    "\n",
    "The data is comprised of three sets of features\n",
    "* Transactions\n",
    "* Users\n",
    "* Merchants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805c9d23",
   "metadata": {},
   "source": [
    "#### To get feature vectors of Transaction nodes, transform the training data using pre-fitted transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "584fe9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_feature_df = pd.DataFrame(\n",
    "    loaded_transformer.transform(\n",
    "        data[predictor_columns].to_pandas()\n",
    "        ),\n",
    "    columns=columns_of_transformed_data).astype(type_mapping)\n",
    "\n",
    "node_feature_df[COL_FRAUD] = data[COL_FRAUD].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aa8f86",
   "metadata": {},
   "source": [
    "#### For graph nodes associated with merchant and user, add feature vectors of zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b35f9f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of graph nodes for users and merchants \n",
    "nr_users_and_merchant_nodes = max_user_id - max_tx_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b5d312bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not spread_features:\n",
    "    # Create feature vector of all zeros for each user and merchant node\n",
    "    empty_feature_df = cudf.DataFrame(\n",
    "        columns=columns_of_transformed_data + target_column,\n",
    "        dtype='int8', \n",
    "        index=range(nr_users_and_merchant_nodes)\n",
    "    )\n",
    "    empty_feature_df = empty_feature_df.fillna(0)\n",
    "    empty_feature_df=empty_feature_df.astype(type_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a72d3ea5-e04f-4af1-a0e0-09964555c1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not spread_features:\n",
    "    # Concatenate transaction features followed by features for merchants and user nodes\n",
    "    node_feature_df = pd.concat([node_feature_df, empty_feature_df.to_pandas()]).astype(type_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a364d173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User specific columns\n",
    "if spread_features:\n",
    "    user_specific_columns = [COL_CARD]\n",
    "    user_specific_columns_of_transformed_data = []\n",
    "\n",
    "    for col in node_feature_df.columns:\n",
    "        if '_'.join(col.split('_')[:-1]) in user_specific_columns:\n",
    "            user_specific_columns_of_transformed_data.append(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "92d88c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merchant specific columns\n",
    "if spread_features:\n",
    "    merchant_specific_columns = [COL_MERCHANT, COL_CITY, COL_ZIP, COL_MCC]\n",
    "    merchant_specific_columns_of_transformed_data = []\n",
    "    \n",
    "    for col in node_feature_df.columns:\n",
    "        if col.split('_')[0] in merchant_specific_columns:\n",
    "            merchant_specific_columns_of_transformed_data.append(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f62755ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction specific columns\n",
    "if spread_features:\n",
    "    transaction_specific_columns = list(\n",
    "        set(numerical_predictors).union(nominal_predictors)\n",
    "        - set(user_specific_columns).union(merchant_specific_columns))\n",
    "    transaction_specific_columns_of_transformed_data = []\n",
    "    \n",
    "    for col in node_feature_df.columns:\n",
    "        if col.split('_')[0] in transaction_specific_columns:\n",
    "            transaction_specific_columns_of_transformed_data.append(col)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12061da",
   "metadata": {},
   "source": [
    "#### Construct feature vector for merchants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "de484a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "if spread_features:\n",
    "    # Find indices of unique merchants\n",
    "    idx_df = cudf.DataFrame()\n",
    "    idx_df[COL_MERCHANT_ID] =  data[COL_MERCHANT_ID]\n",
    "    idx_df = idx_df.sort_values(by=COL_MERCHANT_ID)\n",
    "    idx_df = idx_df.drop_duplicates(subset=COL_MERCHANT_ID)\n",
    "    assert((data.iloc[idx_df.index][COL_MERCHANT_ID] == idx_df[COL_MERCHANT_ID]).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5be790eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if spread_features:\n",
    "    # Copy merchant specific columns, and set the rest to zero\n",
    "    merchant_specific_feature_df = node_feature_df.iloc[idx_df.index.to_numpy()]\n",
    "    merchant_specific_feature_df.\\\n",
    "    loc[:, \n",
    "        transaction_specific_columns_of_transformed_data +\n",
    "          user_specific_columns_of_transformed_data] = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "576091c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if spread_features:\n",
    "    # Find indices of unique users\n",
    "    idx_df = cudf.DataFrame()\n",
    "    idx_df[COL_USER_ID] = data[COL_USER_ID]\n",
    "    idx_df = idx_df.sort_values(by=COL_USER_ID)\n",
    "    idx_df = idx_df.drop_duplicates(subset=COL_USER_ID)\n",
    "    assert((data.iloc[idx_df.index][COL_USER_ID] == idx_df[COL_USER_ID]).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "aec23ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if spread_features:\n",
    "    # Copy user specific columns, and set the rest to zero\n",
    "    user_specific_feature_df = node_feature_df.iloc[idx_df.index.to_numpy()]\n",
    "    user_specific_feature_df.\\\n",
    "    loc[:,\n",
    "        transaction_specific_columns_of_transformed_data +\n",
    "          merchant_specific_columns_of_transformed_data] = 0.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8296a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate features of node, user and merchant\n",
    "if spread_features:\n",
    "    \n",
    "    node_feature_df[merchant_specific_columns_of_transformed_data] = 0.0\n",
    "    node_feature_df[user_specific_columns_of_transformed_data] = 0.0\n",
    "    node_feature_df = pd.concat(\n",
    "        [node_feature_df, merchant_specific_feature_df, user_specific_feature_df]\n",
    "        ).astype(type_mapping)\n",
    "    \n",
    "    # features to save\n",
    "    node_feature_df = node_feature_df[\n",
    "        transaction_specific_columns_of_transformed_data +\n",
    "        merchant_specific_columns_of_transformed_data +\n",
    "        user_specific_columns_of_transformed_data + [COL_FRAUD]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54aa686",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_feature_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "527f6ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# target labels to save\n",
    "label_df = node_feature_df[[COL_FRAUD]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "15e1cba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove target label from feature vectors\n",
    "_ = node_feature_df.drop(columns=[COL_FRAUD], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310d9500",
   "metadata": {},
   "source": [
    "#### Write out node features and target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "139bfd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write node target label to csv file\n",
    "out_path = os.path.join(sparkov_gnn, 'labels.csv')\n",
    "\n",
    "if not os.path.exists(os.path.dirname(out_path)):\n",
    "  os.makedirs(os.path.dirname(out_path))\n",
    "\n",
    "label_df.to_csv(out_path, header=False, index=False)\n",
    "# label_df.to_parquet(out_path, index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b8fe801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write node features to csv file\n",
    "out_path = os.path.join(sparkov_gnn, 'features.csv')\n",
    "\n",
    "if not os.path.exists(os.path.dirname(out_path)):\n",
    "  os.makedirs(os.path.dirname(out_path))\n",
    "node_feature_df[columns_of_transformed_data].to_csv(out_path, header=True, index=False)\n",
    "# node_feature_df.to_parquet(out_path, index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fbe75d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete dataFrames\n",
    "del data\n",
    "del node_feature_df\n",
    "del label_df\n",
    "\n",
    "if spread_features:\n",
    "    del merchant_specific_feature_df\n",
    "    del user_specific_feature_df\n",
    "else:\n",
    "    del empty_feature_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657362a9",
   "metadata": {},
   "source": [
    "#### Number of transaction nodes in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b9ccd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of transaction nodes, needed for GNN training\n",
    "nr_transaction_nodes = max_tx_id + 1\n",
    "nr_transaction_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fce29ee",
   "metadata": {},
   "source": [
    "#### Save variable for training and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c3bf9b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "variables_to_save = {\n",
    "    k: v for k, v in globals().items() if isinstance(v, (str, int)) and k.startswith('COL_')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "54cc3c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_to_save['NUM_TRANSACTION_NODES'] = int(nr_transaction_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9eb8bdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary to a JSON file\n",
    "with open(os.path.join(sparkov_base_path, 'variables.json'), 'w') as json_file:\n",
    "    json.dump(variables_to_save, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2f6f28",
   "metadata": {},
   "source": [
    "## That's it!\n",
    "The data is now ready for processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c13b3b",
   "metadata": {},
   "source": [
    "## Copyright and License\n",
    "<hr/>\n",
    "Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.\n",
    "\n",
    "<br/>\n",
    "\n",
    " Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    " you may not use this file except in compliance with the License.\n",
    " You may obtain a copy of the License at\n",
    " \n",
    " http://www.apache.org/licenses/LICENSE-2.0\n",
    " \n",
    " Unless required by applicable law or agreed to in writing, software\n",
    " distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    " WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    " See the License for the specific language governing permissions and\n",
    " limitations under the License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
